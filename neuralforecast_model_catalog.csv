full_path,Object_Name,Type,Summary,Arguments_List,Num_Arguments
neuralforecast.auto.AutoAutoformer,AutoAutoformer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoBiTCN,AutoBiTCN,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoDLinear,AutoDLinear,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoDeepAR,AutoDeepAR,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoDeepNPTS,AutoDeepNPTS,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoDilatedRNN,AutoDilatedRNN,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoFEDformer,AutoFEDformer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoGRU,AutoGRU,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoHINT,AutoHINT,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","S, alias, backend, callbacks, cls_model, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",15
neuralforecast.auto.AutoInformer,AutoInformer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoKAN,AutoKAN,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoLSTM,AutoLSTM,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoMLP,AutoMLP,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoMLPMultivariate,AutoMLPMultivariate,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoNBEATS,AutoNBEATS,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoNBEATSx,AutoNBEATSx,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoNHITS,AutoNHITS,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoNLinear,AutoNLinear,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoPatchTST,AutoPatchTST,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoRMoK,AutoRMoK,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoRNN,AutoRNN,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoSOFTS,AutoSOFTS,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoStemGNN,AutoStemGNN,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoTCN,AutoTCN,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoTFT,AutoTFT,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoTSMixer,AutoTSMixer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoTSMixerx,AutoTSMixerx,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoTiDE,AutoTiDE,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoTimeMixer,AutoTimeMixer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoTimeXer,AutoTimeXer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoTimesNet,AutoTimesNet,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoVanillaTransformer,AutoVanillaTransformer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.auto.AutoiTransformer,AutoiTransformer,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, n_series, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.auto.AutoxLSTM,AutoxLSTM,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",13
neuralforecast.common._base_auto.BaseAuto,BaseAuto,class,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to","alias, backend, callbacks, cls_model, config, cpus, gpus, h, loss, num_samples, refit_with_val, search_alg, valid_loss, verbose",14
neuralforecast.common._base_auto.MockTrial,MockTrial,class,,"args, kwargs",2
neuralforecast.common._base_model.BaseModel,BaseModel,class,Hooks to be used in LightningModule.,"alias, batch_size, dataloader_kwargs, drop_last_loader, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, h_train, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_samples, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",34
neuralforecast.common._base_model.DistributedConfig,DistributedConfig,class,"DistributedConfig(partitions_path: str, num_nodes: int, devices: int)","devices, num_nodes, partitions_path",3
neuralforecast.common._base_model.tensor_to_numpy,tensor_to_numpy,function,Convert a tensor to numpy,tensor,1
neuralforecast.common._model_checks.check_airpassengers,check_airpassengers,function,,model_class,1
neuralforecast.common._model_checks.check_loss_functions,check_loss_functions,function,,model_class,1
neuralforecast.common._model_checks.check_model,check_model,function,Check model with various tests. Options for checks are:,"checks, model_class",2
neuralforecast.common._modules.AttentionLayer,AttentionLayer,class,Base class for all neural network modules.,"attention, d_keys, d_values, hidden_size, n_heads",5
neuralforecast.common._modules.CausalConv1d,CausalConv1d,class,Causal Convolution 1d,"activation, dilation, in_channels, kernel_size, out_channels, padding, stride",7
neuralforecast.common._modules.Chomp1d,Chomp1d,class,Chomp1d,horizon,1
neuralforecast.common._modules.DataEmbedding,DataEmbedding,class,Base class for all neural network modules.,"c_in, dropout, exog_input_size, hidden_size, pos_embedding",5
neuralforecast.common._modules.DataEmbedding_inverted,DataEmbedding_inverted,class,DataEmbedding_inverted,"c_in, dropout, hidden_size",3
neuralforecast.common._modules.FixedEmbedding,FixedEmbedding,class,Base class for all neural network modules.,"c_in, d_model",2
neuralforecast.common._modules.FullAttention,FullAttention,class,Base class for all neural network modules.,"attention_dropout, factor, mask_flag, output_attention, scale",5
neuralforecast.common._modules.MLP,MLP,class,Multi-Layer Perceptron Class,"activation, dropout, hidden_size, in_features, num_layers, out_features",6
neuralforecast.common._modules.MovingAvg,MovingAvg,class,Moving average block to highlight the trend of time series,"kernel_size, stride",2
neuralforecast.common._modules.PositionalEmbedding,PositionalEmbedding,class,Base class for all neural network modules.,"hidden_size, max_len",2
neuralforecast.common._modules.RevIN,RevIN,class,RevIN (Reversible-Instance-Normalization),"affine, eps, non_norm, num_features, subtract_last",5
neuralforecast.common._modules.RevINMultivariate,RevINMultivariate,class,ReversibleInstanceNorm1d for Multivariate models,"affine, eps, non_norm, num_features, subtract_last",5
neuralforecast.common._modules.SeriesDecomp,SeriesDecomp,class,Series decomposition block,kernel_size,1
neuralforecast.common._modules.TemporalConvolutionEncoder,TemporalConvolutionEncoder,class,Temporal Convolution Encoder,"activation, dilations, in_channels, kernel_size, out_channels",5
neuralforecast.common._modules.TemporalEmbedding,TemporalEmbedding,class,Base class for all neural network modules.,"d_model, embed_type, freq",3
neuralforecast.common._modules.TimeFeatureEmbedding,TimeFeatureEmbedding,class,Base class for all neural network modules.,"hidden_size, input_size",2
neuralforecast.common._modules.TokenEmbedding,TokenEmbedding,class,Base class for all neural network modules.,"c_in, hidden_size",2
neuralforecast.common._modules.TransDecoder,TransDecoder,class,Base class for all neural network modules.,"layers, norm_layer, projection",3
neuralforecast.common._modules.TransDecoderLayer,TransDecoderLayer,class,Base class for all neural network modules.,"activation, conv_hidden_size, cross_attention, dropout, hidden_size, self_attention",6
neuralforecast.common._modules.TransEncoder,TransEncoder,class,Base class for all neural network modules.,"attn_layers, conv_layers, norm_layer",3
neuralforecast.common._modules.TransEncoderLayer,TransEncoderLayer,class,Base class for all neural network modules.,"activation, attention, conv_hidden_size, dropout, hidden_size",5
neuralforecast.common._modules.TriangularCausalMask,TriangularCausalMask,class,TriangularCausalMask,"B, L, device",3
neuralforecast.common._scalers.TemporalNorm,TemporalNorm,class,Temporal Normalization,"dim, eps, num_features, scaler_type",4
neuralforecast.common._scalers.identity_scaler,identity_scaler,function,,"x, x_scale, x_shift",3
neuralforecast.common._scalers.identity_statistics,identity_statistics,function,Identity Scaler,"dim, eps, mask, x",4
neuralforecast.common._scalers.inv_identity_scaler,inv_identity_scaler,function,,"x_scale, x_shift, z",3
neuralforecast.common._scalers.inv_invariant_scaler,inv_invariant_scaler,function,,"x_mad, x_median, z",3
neuralforecast.common._scalers.inv_minmax1_scaler,inv_minmax1_scaler,function,,"x_min, x_range, z",3
neuralforecast.common._scalers.inv_minmax_scaler,inv_minmax_scaler,function,,"x_min, x_range, z",3
neuralforecast.common._scalers.inv_robust_scaler,inv_robust_scaler,function,,"x_mad, x_median, z",3
neuralforecast.common._scalers.inv_std_scaler,inv_std_scaler,function,,"x_mean, x_std, z",3
neuralforecast.common._scalers.invariant_scaler,invariant_scaler,function,,"x, x_mad, x_median",3
neuralforecast.common._scalers.invariant_statistics,invariant_statistics,function,Invariant Median Scaler,"dim, eps, mask, x",4
neuralforecast.common._scalers.masked_mean,masked_mean,function,Masked  Mean,"dim, keepdim, mask, x",4
neuralforecast.common._scalers.masked_median,masked_median,function,Masked Median,"dim, keepdim, mask, x",4
neuralforecast.common._scalers.minmax1_scaler,minmax1_scaler,function,,"x, x_min, x_range",3
neuralforecast.common._scalers.minmax1_statistics,minmax1_statistics,function,MinMax1 Scaler,"dim, eps, mask, x",4
neuralforecast.common._scalers.minmax_scaler,minmax_scaler,function,,"x, x_min, x_range",3
neuralforecast.common._scalers.minmax_statistics,minmax_statistics,function,MinMax Scaler,"dim, eps, mask, x",4
neuralforecast.common._scalers.robust_scaler,robust_scaler,function,,"x, x_mad, x_median",3
neuralforecast.common._scalers.robust_statistics,robust_statistics,function,Robust Median Scaler,"dim, eps, mask, x",4
neuralforecast.common._scalers.std_scaler,std_scaler,function,,"x, x_means, x_stds",3
neuralforecast.common._scalers.std_statistics,std_statistics,function,Standard Scaler,"dim, eps, mask, x",4
neuralforecast.common.enums.ExplainerEnum,ExplainerEnum,class,str(object='') -> str,"args, kwds",2
neuralforecast.common.enums.TimeSeriesDatasetEnum,TimeSeriesDatasetEnum,class,str(object='') -> str,"args, kwds",2
neuralforecast.compat.SparkDataFrame,SparkDataFrame,class,,"args, kwargs",2
neuralforecast.core.NeuralForecast,NeuralForecast,class,,"freq, local_scaler_type, models",3
neuralforecast.losses.numpy.mae,mae,function,Mean Absolute Error,"axis, weights, y, y_hat",4
neuralforecast.losses.numpy.mape,mape,function,Mean Absolute Percentage Error,"axis, weights, y, y_hat",4
neuralforecast.losses.numpy.mase,mase,function,Mean Absolute Scaled Error,"axis, seasonality, weights, y, y_hat, y_train",6
neuralforecast.losses.numpy.mqloss,mqloss,function,Multi-Quantile loss,"axis, quantiles, weights, y, y_hat",5
neuralforecast.losses.numpy.mse,mse,function,Mean Squared Error,"axis, weights, y, y_hat",4
neuralforecast.losses.numpy.quantile_loss,quantile_loss,function,Quantile Loss,"axis, q, weights, y, y_hat",5
neuralforecast.losses.numpy.rmae,rmae,function,RMAE,"axis, weights, y, y_hat1, y_hat2",5
neuralforecast.losses.numpy.rmse,rmse,function,Root Mean Squared Error,"axis, weights, y, y_hat",4
neuralforecast.losses.numpy.smape,smape,function,Symmetric Mean Absolute Percentage Error,"axis, weights, y, y_hat",4
neuralforecast.losses.pytorch.Accuracy,Accuracy,class,Accuracy,,0
neuralforecast.losses.pytorch.BaseISQF,BaseISQF,class,Base distribution class for the Incremental (Spline) Quantile Function.,"beta_l, beta_r, qk_x, qk_y, spline_heights, spline_knots, tol, validate_args",8
neuralforecast.losses.pytorch.BasePointLoss,BasePointLoss,class,Base class for point loss functions.,"horizon_weight, output_names, outputsize_multiplier",3
neuralforecast.losses.pytorch.DistributionLoss,DistributionLoss,class,DistributionLoss,"distribution, distribution_kwargs, horizon_weight, level, num_samples, quantiles, return_params",7
neuralforecast.losses.pytorch.GMM,GMM,class,Gaussian Mixture Mesh,"batch_correlation, horizon_correlation, level, n_components, num_samples, quantiles, return_params, weighted",8
neuralforecast.losses.pytorch.HuberIQLoss,HuberIQLoss,class,Implicit Huber Quantile Loss,"concentration0, concentration1, cos_embedding_dim, delta, horizon_weight",5
neuralforecast.losses.pytorch.HuberLoss,HuberLoss,class,Huber Loss,"delta, horizon_weight",2
neuralforecast.losses.pytorch.HuberMQLoss,HuberMQLoss,class,Huberized Multi-Quantile loss,"delta, horizon_weight, level, quantiles",4
neuralforecast.losses.pytorch.HuberQLoss,HuberQLoss,class,Huberized Quantile Loss,"delta, horizon_weight, q",3
neuralforecast.losses.pytorch.IQLoss,IQLoss,class,Implicit Quantile Loss.,"concentration0, concentration1, cos_embedding_dim, horizon_weight",4
neuralforecast.losses.pytorch.ISQF,ISQF,class,Distribution class for the Incremental (Spline) Quantile Function.,"beta_l, beta_r, loc, qk_x, qk_y, scale, spline_heights, spline_knots, validate_args",9
neuralforecast.losses.pytorch.MAE,MAE,class,Mean Absolute Error.,horizon_weight,1
neuralforecast.losses.pytorch.MAPE,MAPE,class,Mean Absolute Percentage Error,horizon_weight,1
neuralforecast.losses.pytorch.MASE,MASE,class,Mean Absolute Scaled Error,"horizon_weight, seasonality",2
neuralforecast.losses.pytorch.MQLoss,MQLoss,class,Multi-Quantile loss,"horizon_weight, level, quantiles",3
neuralforecast.losses.pytorch.MSE,MSE,class,Mean Squared Error.,horizon_weight,1
neuralforecast.losses.pytorch.NBMM,NBMM,class,Negative Binomial Mixture Mesh,"level, n_components, num_samples, quantiles, return_params, weighted",6
neuralforecast.losses.pytorch.PMM,PMM,class,Poisson Mixture Mesh,"batch_correlation, horizon_correlation, level, n_components, num_samples, quantiles, return_params, weighted",8
neuralforecast.losses.pytorch.QuantileLayer,QuantileLayer,class,Implicit Quantile Layer from the paper IQN for Distributional Reinforcement Learning.,"cos_embedding_dim, num_output",2
neuralforecast.losses.pytorch.QuantileLoss,QuantileLoss,class,Quantile Loss.,"horizon_weight, q",2
neuralforecast.losses.pytorch.RMSE,RMSE,class,Root Mean Squared Error.,horizon_weight,1
neuralforecast.losses.pytorch.SMAPE,SMAPE,class,Symmetric Mean Absolute Percentage Error,horizon_weight,1
neuralforecast.losses.pytorch.TukeyLoss,TukeyLoss,class,Tukey Loss,"c, normalize",2
neuralforecast.losses.pytorch.Tweedie,Tweedie,class,Tweedie Distribution.,"log_mu, rho, validate_args",3
neuralforecast.losses.pytorch.bernoulli_scale_decouple,bernoulli_scale_decouple,function,Bernoulli Scale Decouple.,"loc, output, scale",3
neuralforecast.losses.pytorch.est_alpha,est_alpha,function,,rho,1
neuralforecast.losses.pytorch.est_beta,est_beta,function,,"mu, rho",2
neuralforecast.losses.pytorch.est_lambda,est_lambda,function,,"mu, rho",2
neuralforecast.losses.pytorch.isqf_domain_map,isqf_domain_map,function,ISQF Domain Map,"input, num_pieces, quantiles, tol",4
neuralforecast.losses.pytorch.isqf_scale_decouple,isqf_scale_decouple,function,ISQF Scale Decouple,"loc, output, scale",3
neuralforecast.losses.pytorch.level_to_outputs,level_to_outputs,function,,level,1
neuralforecast.losses.pytorch.nbinomial_scale_decouple,nbinomial_scale_decouple,function,Negative Binomial Scale Decouple,"loc, output, scale",3
neuralforecast.losses.pytorch.normal_scale_decouple,normal_scale_decouple,function,Normal Scale Decouple.,"eps, loc, output, scale",4
neuralforecast.losses.pytorch.poisson_scale_decouple,poisson_scale_decouple,function,Poisson Scale Decouple,"loc, output, scale",3
neuralforecast.losses.pytorch.quantiles_to_outputs,quantiles_to_outputs,function,,quantiles,1
neuralforecast.losses.pytorch.relMSE,relMSE,class,Relative Mean Squared Error,"horizon_weight, y_train",2
neuralforecast.losses.pytorch.sCRPS,sCRPS,class,Scaled Continues Ranked Probability Score,"level, quantiles",2
neuralforecast.losses.pytorch.student_scale_decouple,student_scale_decouple,function,Student-T Scale Decouple.,"eps, loc, output, scale",4
neuralforecast.losses.pytorch.tweedie_domain_map,tweedie_domain_map,function,Maps output of neural network to domain of distribution loss,"input, rho",2
neuralforecast.losses.pytorch.tweedie_scale_decouple,tweedie_scale_decouple,function,Tweedie Scale Decouple,"loc, output, scale",3
neuralforecast.losses.pytorch.weighted_average,weighted_average,function,Computes the weighted average of a given tensor across a given dim.,"dim, weights, x",3
neuralforecast.models.autoformer.AutoCorrelation,AutoCorrelation,class,AutoCorrelation Mechanism with the following two phases:,"attention_dropout, factor, mask_flag, output_attention, scale",5
neuralforecast.models.autoformer.AutoCorrelationLayer,AutoCorrelationLayer,class,Auto Correlation Layer,"correlation, d_keys, d_values, hidden_size, n_head",5
neuralforecast.models.autoformer.Autoformer,Autoformer,class,Autoformer,"MovingAvg_window, activation, alias, batch_size, conv_hidden_size, dataloader_kwargs, decoder_input_size_multiplier, decoder_layers, drop_last_loader, dropout, early_stop_patience_steps, encoder_layers, exclude_insample_y, factor, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_head, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",40
neuralforecast.models.autoformer.Decoder,Decoder,class,Autoformer decoder,"layers, norm_layer, projection",3
neuralforecast.models.autoformer.DecoderLayer,DecoderLayer,class,Autoformer decoder layer with the progressive decomposition architecture,"MovingAvg, activation, c_out, conv_hidden_size, cross_attention, dropout, hidden_size, self_attention",8
neuralforecast.models.autoformer.Encoder,Encoder,class,Autoformer encoder,"attn_layers, conv_layers, norm_layer",3
neuralforecast.models.autoformer.EncoderLayer,EncoderLayer,class,Autoformer encoder layer with the progressive decomposition architecture,"MovingAvg, activation, attention, conv_hidden_size, dropout, hidden_size",6
neuralforecast.models.autoformer.LayerNorm,LayerNorm,class,Special designed layernorm for the seasonal part,channels,1
neuralforecast.models.bitcn.BiTCN,BiTCN,class,BiTCN,"alias, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",32
neuralforecast.models.bitcn.CustomConv1d,CustomConv1d,class,Forward- and backward looking Conv1D,"dilation, groups, in_channels, kernel_size, mode, out_channels, padding",7
neuralforecast.models.bitcn.TCNCell,TCNCell,class,"Temporal Convolutional Network Cell, consisting of CustomConv1D modules.","dilation, dropout, groups, in_channels, kernel_size, mode, out_channels, padding",8
neuralforecast.models.deepar.Decoder,Decoder,class,Multi-Layer Perceptron Decoder,"hidden_layers, hidden_size, in_features, out_features",4
neuralforecast.models.deepar.DeepAR,DeepAR,class,DeepAR,"alias, batch_size, dataloader_kwargs, decoder_hidden_layers, decoder_hidden_size, drop_last_loader, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, h_train, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, lstm_dropout, lstm_hidden_size, lstm_n_layers, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, trajectory_samples, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",37
neuralforecast.models.deepnpts.DeepNPTS,DeepNPTS,class,DeepNPTS,"alias, batch_norm, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_layers, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",34
neuralforecast.models.dilated_rnn.AttentiveLSTMLayer,AttentiveLSTMLayer,class,Base class for all neural network modules.,"dropout, hidden_size, input_size",3
neuralforecast.models.dilated_rnn.DRNN,DRNN,class,Base class for all neural network modules.,"batch_first, cell_type, dilations, dropout, n_hidden, n_input, n_layers",7
neuralforecast.models.dilated_rnn.DilatedRNN,DilatedRNN,class,DilatedRNN,"alias, batch_size, cell_type, context_size, dataloader_kwargs, decoder_hidden_size, decoder_layers, dilations, drop_last_loader, early_stop_patience_steps, encoder_hidden_size, exclude_insample_y, futr_exog_list, h, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",37
neuralforecast.models.dilated_rnn.LSTMCell,LSTMCell,class,Base class for all neural network modules.,"dropout, hidden_size, input_size",3
neuralforecast.models.dilated_rnn.ResLSTMCell,ResLSTMCell,class,Base class for all neural network modules.,"dropout, hidden_size, input_size",3
neuralforecast.models.dilated_rnn.ResLSTMLayer,ResLSTMLayer,class,Base class for all neural network modules.,"dropout, hidden_size, input_size",3
neuralforecast.models.dlinear.DLinear,DLinear,class,DLinear,"alias, batch_size, dataloader_kwargs, drop_last_loader, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, moving_avg_window, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",31
neuralforecast.models.dlinear.MovingAvg,MovingAvg,class,Moving average block to highlight the trend of time series,"kernel_size, stride",2
neuralforecast.models.dlinear.SeriesDecomp,SeriesDecomp,class,Series decomposition block,kernel_size,1
neuralforecast.models.fedformer.AutoCorrelationLayer,AutoCorrelationLayer,class,Auto Correlation Layer,"correlation, d_keys, d_values, hidden_size, n_head",5
neuralforecast.models.fedformer.Decoder,Decoder,class,FEDformer decoder,"layers, norm_layer, projection",3
neuralforecast.models.fedformer.DecoderLayer,DecoderLayer,class,FEDformer decoder layer with the progressive decomposition architecture,"MovingAvg, activation, c_out, conv_hidden_size, cross_attention, dropout, hidden_size, self_attention",8
neuralforecast.models.fedformer.Encoder,Encoder,class,FEDformer encoder,"attn_layers, conv_layers, norm_layer",3
neuralforecast.models.fedformer.EncoderLayer,EncoderLayer,class,FEDformer encoder layer with the progressive decomposition architecture,"MovingAvg, activation, attention, conv_hidden_size, dropout, hidden_size",6
neuralforecast.models.fedformer.FEDformer,FEDformer,class,FEDformer,"MovingAvg_window, activation, alias, batch_size, conv_hidden_size, dataloader_kwargs, decoder_input_size_multiplier, decoder_layers, drop_last_loader, dropout, early_stop_patience_steps, encoder_layers, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, mode_select, modes, n_head, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, version, windows_batch_size",41
neuralforecast.models.fedformer.FourierBlock,FourierBlock,class,Fourier block,"in_channels, mode_select_method, modes, out_channels, seq_len",5
neuralforecast.models.fedformer.FourierCrossAttention,FourierCrossAttention,class,Fourier Cross Attention layer,"activation, in_channels, mode_select_method, modes, out_channels, policy, seq_len_kv, seq_len_q",8
neuralforecast.models.fedformer.LayerNorm,LayerNorm,class,Special designed layernorm for the seasonal part,channels,1
neuralforecast.models.fedformer.get_frequency_modes,get_frequency_modes,function,Get modes on frequency domain:,"mode_select_method, modes, seq_len",3
neuralforecast.models.gru.GRU,GRU,class,GRU,"alias, batch_size, context_size, dataloader_kwargs, decoder_hidden_size, decoder_layers, drop_last_loader, early_stop_patience_steps, encoder_activation, encoder_bias, encoder_dropout, encoder_hidden_size, encoder_n_layers, exclude_insample_y, futr_exog_list, h, h_train, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, recurrent, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",41
neuralforecast.models.hint.HINT,HINT,class,HINT,"S, alias, h, model, reconciliation",5
neuralforecast.models.hint.get_bottomup_P,get_bottomup_P,function,BottomUp Reconciliation Matrix.,S,1
neuralforecast.models.hint.get_identity_P,get_identity_P,function,,S,1
neuralforecast.models.hint.get_mintrace_ols_P,get_mintrace_ols_P,function,MinTraceOLS Reconciliation Matrix.,S,1
neuralforecast.models.hint.get_mintrace_wls_P,get_mintrace_wls_P,function,MinTraceOLS Reconciliation Matrix.,S,1
neuralforecast.models.informer.ConvLayer,ConvLayer,class,ConvLayer,c_in,1
neuralforecast.models.informer.Informer,Informer,class,Informer,"activation, alias, batch_size, conv_hidden_size, dataloader_kwargs, decoder_input_size_multiplier, decoder_layers, distil, drop_last_loader, dropout, early_stop_patience_steps, encoder_layers, exclude_insample_y, factor, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_head, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",40
neuralforecast.models.informer.ProbAttention,ProbAttention,class,ProbAttention,"attention_dropout, factor, mask_flag, output_attention, scale",5
neuralforecast.models.informer.ProbMask,ProbMask,class,ProbMask,"B, H, L, device, index, scores",6
neuralforecast.models.itransformer.iTransformer,iTransformer,class,iTransformer,"alias, batch_size, d_ff, d_layers, dataloader_kwargs, drop_last_loader, dropout, e_layers, early_stop_patience_steps, exclude_insample_y, factor, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_heads, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, use_norm, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",39
neuralforecast.models.kan.KAN,KAN,class,KAN,"alias, batch_size, dataloader_kwargs, drop_last_loader, early_stop_patience_steps, enable_standalone_scale_spline, exclude_insample_y, futr_exog_list, grid_eps, grid_range, grid_size, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, max_steps, n_hidden_layers, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scale_base, scale_noise, scale_spline, scaler_type, spline_order, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",38
neuralforecast.models.kan.KANLinear,KANLinear,class,KANLinear,"base_activation, enable_standalone_scale_spline, grid_eps, grid_range, grid_size, in_features, out_features, scale_base, scale_noise, scale_spline, spline_order",11
neuralforecast.models.lstm.LSTM,LSTM,class,LSTM,"alias, batch_size, context_size, dataloader_kwargs, decoder_hidden_size, decoder_layers, drop_last_loader, early_stop_patience_steps, encoder_bias, encoder_dropout, encoder_hidden_size, encoder_n_layers, exclude_insample_y, futr_exog_list, h, h_train, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, recurrent, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",40
neuralforecast.models.mlp.MLP,MLP,class,MLP,"alias, batch_size, dataloader_kwargs, drop_last_loader, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_layers, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",32
neuralforecast.models.mlpmultivariate.MLPMultivariate,MLPMultivariate,class,MLPMultivariate,"alias, batch_size, dataloader_kwargs, drop_last_loader, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_series, num_layers, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",33
neuralforecast.models.nbeats.IdentityBasis,IdentityBasis,class,Base class for all neural network modules.,"backcast_size, forecast_size, out_features",3
neuralforecast.models.nbeats.NBEATS,NBEATS,class,NBEATS,"activation, alias, basis, batch_size, dataloader_kwargs, drop_last_loader, dropout_prob_theta, early_stop_patience_steps, h, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, mlp_units, n_basis, n_blocks, n_harmonics, n_polynomials, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, shared_weights, stack_types, start_padding_enabled, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",36
neuralforecast.models.nbeats.NBEATSBlock,NBEATSBlock,class,N-BEATS block which takes a basis function as an argument.,"activation, basis, dropout_prob, input_size, mlp_units, n_theta",6
neuralforecast.models.nbeats.SeasonalityBasis,SeasonalityBasis,class,Base class for all neural network modules.,"backcast_size, forecast_size, harmonics, out_features",4
neuralforecast.models.nbeats.TrendBasis,TrendBasis,class,Base class for all neural network modules.,"backcast_size, basis, forecast_size, n_basis, out_features",5
neuralforecast.models.nbeats.generate_changepoint_basis,generate_changepoint_basis,function,Generates changepoint basis functions with automatically spaced changepoints.,"length, n_basis",2
neuralforecast.models.nbeats.generate_chebyshev_basis,generate_chebyshev_basis,function,Generates Chebyshev polynomial basis functions.,"length, n_basis",2
neuralforecast.models.nbeats.generate_legendre_basis,generate_legendre_basis,function,Generates Legendre polynomial basis functions.,"length, n_basis",2
neuralforecast.models.nbeats.generate_linear_hat_basis,generate_linear_hat_basis,function,,"length, n_basis",2
neuralforecast.models.nbeats.generate_piecewise_linear_basis,generate_piecewise_linear_basis,function,Generates piecewise linear basis functions (linear splines).,"length, n_basis",2
neuralforecast.models.nbeats.generate_polynomial_basis,generate_polynomial_basis,function,Generates standard polynomial basis functions.,"length, n_basis",2
neuralforecast.models.nbeats.generate_spline_basis,generate_spline_basis,function,Generates cubic spline basis functions.,"length, n_basis",2
neuralforecast.models.nbeats.get_basis,get_basis,function,,"basis, length, n_basis",3
neuralforecast.models.nbeatsx.ExogenousBasis,ExogenousBasis,class,Base class for all neural network modules.,forecast_size,1
neuralforecast.models.nbeatsx.IdentityBasis,IdentityBasis,class,Base class for all neural network modules.,"backcast_size, forecast_size, out_features",3
neuralforecast.models.nbeatsx.NBEATSBlock,NBEATSBlock,class,N-BEATS block which takes a basis function as an argument.,"activation, basis, dropout_prob, futr_input_size, h, hist_input_size, input_size, mlp_units, n_theta, stat_input_size",10
neuralforecast.models.nbeatsx.NBEATSx,NBEATSx,class,NBEATSx,"activation, alias, batch_size, dataloader_kwargs, drop_last_loader, dropout_prob_theta, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, mlp_units, n_blocks, n_harmonics, n_polynomials, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, shared_weights, stack_types, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",38
neuralforecast.models.nbeatsx.SeasonalityBasis,SeasonalityBasis,class,Base class for all neural network modules.,"backcast_size, forecast_size, harmonics, out_features",4
neuralforecast.models.nbeatsx.TrendBasis,TrendBasis,class,Base class for all neural network modules.,"backcast_size, degree_of_polynomial, forecast_size, out_features",4
neuralforecast.models.nhits.NHITS,NHITS,class,NHITS,"activation, alias, batch_size, dataloader_kwargs, drop_last_loader, dropout_prob_theta, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, interpolation_mode, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, mlp_units, n_blocks, n_freq_downsample, n_pool_kernel_size, num_lr_decays, optimizer, optimizer_kwargs, pooling_mode, random_seed, scaler_type, stack_types, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",39
neuralforecast.models.nhits.NHITSBlock,NHITSBlock,class,NHITS block which takes a basis function as an argument.,"activation, basis, dropout_prob, futr_input_size, h, hist_input_size, input_size, mlp_units, n_pool_kernel_size, n_theta, pooling_mode, stat_input_size",12
neuralforecast.models.nlinear.NLinear,NLinear,class,NLinear,"alias, batch_size, dataloader_kwargs, drop_last_loader, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",30
neuralforecast.models.patchtst.Coord1dPosEncoding,Coord1dPosEncoding,function,,"exponential, normalize, q_len",3
neuralforecast.models.patchtst.Coord2dPosEncoding,Coord2dPosEncoding,function,,"eps, exponential, hidden_size, normalize, q_len",5
neuralforecast.models.patchtst.Flatten_Head,Flatten_Head,class,Flatten_Head,"c_out, h, head_dropout, individual, n_vars, nf",6
neuralforecast.models.patchtst.PatchTST,PatchTST,class,PatchTST,"activation, alias, attn_dropout, batch_normalization, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, encoder_layers, exclude_insample_y, fc_dropout, futr_exog_list, h, head_dropout, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learn_pos_embed, learning_rate, linear_hidden_size, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_heads, num_lr_decays, optimizer, optimizer_kwargs, patch_len, random_seed, res_attention, revin, revin_affine, revin_subtract_last, scaler_type, start_padding_enabled, stat_exog_list, step_size, stride, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",47
neuralforecast.models.patchtst.PatchTST_backbone,PatchTST_backbone,class,PatchTST_backbone,"act, affine, attn_dropout, attn_mask, c_in, c_out, d_k, d_v, dropout, fc_dropout, h, head_dropout, head_type, hidden_size, individual, input_size, key_padding_mask, learn_pe, linear_hidden_size, max_seq_len, n_heads, n_layers, norm, padding_patch, padding_var, patch_len, pe, pre_norm, pretrain_head, res_attention, revin, store_attn, stride, subtract_last",34
neuralforecast.models.patchtst.PositionalEncoding,PositionalEncoding,function,,"hidden_size, normalize, q_len",3
neuralforecast.models.patchtst.SinCosPosEncoding,SinCosPosEncoding,function,,"hidden_size, normalize, q_len",3
neuralforecast.models.patchtst.TSTEncoder,TSTEncoder,class,TSTEncoder,"activation, attn_dropout, d_k, d_v, dropout, hidden_size, linear_hidden_size, n_heads, n_layers, norm, pre_norm, q_len, res_attention, store_attn",14
neuralforecast.models.patchtst.TSTEncoderLayer,TSTEncoderLayer,class,TSTEncoderLayer,"activation, attn_dropout, bias, d_k, d_v, dropout, hidden_size, linear_hidden_size, n_heads, norm, pre_norm, q_len, res_attention, store_attn",14
neuralforecast.models.patchtst.TSTiEncoder,TSTiEncoder,class,TSTiEncoder,"act, attn_dropout, attn_mask, c_in, d_k, d_v, dropout, hidden_size, key_padding_mask, learn_pe, linear_hidden_size, max_seq_len, n_heads, n_layers, norm, padding_var, patch_len, patch_num, pe, pre_norm, res_attention, store_attn",22
neuralforecast.models.patchtst.Transpose,Transpose,class,Transpose,"contiguous, dims",2
neuralforecast.models.patchtst.get_activation_fn,get_activation_fn,function,,activation,1
neuralforecast.models.patchtst.positional_encoding,positional_encoding,function,,"hidden_size, learn_pe, pe, q_len",4
neuralforecast.models.rmok.JacobiKANLayer,JacobiKANLayer,class,https://github.com/SpaceLearner/JacobiKAN/blob/main/JacobiKANLayer.py,"a, b, degree, input_dim, output_dim",5
neuralforecast.models.rmok.RMoK,RMoK,class,Reversible Mixture of KAN,"alias, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, jacobi_degree, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, revin_affine, scaler_type, start_padding_enabled, stat_exog_list, step_size, taylor_order, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, wavelet_function, windows_batch_size",35
neuralforecast.models.rmok.TaylorKANLayer,TaylorKANLayer,class,https://github.com/Muyuzhierchengse/TaylorKAN/,"addbias, input_dim, order, out_dim",4
neuralforecast.models.rmok.WaveKANLayer,WaveKANLayer,class,This is a sample code for the simulations of the paper:,"device, in_features, out_features, wavelet_type, with_bn",5
neuralforecast.models.rnn.RNN,RNN,class,RNN,"alias, batch_size, context_size, dataloader_kwargs, decoder_hidden_size, decoder_layers, drop_last_loader, early_stop_patience_steps, encoder_activation, encoder_bias, encoder_dropout, encoder_hidden_size, encoder_n_layers, exclude_insample_y, futr_exog_list, h, h_train, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, recurrent, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",41
neuralforecast.models.softs.DataEmbedding_inverted,DataEmbedding_inverted,class,Data Embedding,"c_in, d_model, dropout",3
neuralforecast.models.softs.SOFTS,SOFTS,class,SOFTS,"alias, batch_size, d_core, d_ff, dataloader_kwargs, drop_last_loader, dropout, e_layers, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, use_norm, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",37
neuralforecast.models.softs.STAD,STAD,class,STar Aggregate Dispatch Module,"d_core, d_series",2
neuralforecast.models.stemgnn.GLU,GLU,class,GLU,"input_channel, output_channel",2
neuralforecast.models.stemgnn.StemGNN,StemGNN,class,StemGNN,"alias, batch_size, dataloader_kwargs, drop_last_loader, dropout_rate, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, leaky_rate, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, multi_layer, n_series, n_stacks, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",35
neuralforecast.models.stemgnn.StockBlockLayer,StockBlockLayer,class,StockBlockLayer,"multi_layer, stack_cnt, time_step, unit",4
neuralforecast.models.tcn.TCN,TCN,class,TCN,"alias, batch_size, context_size, dataloader_kwargs, decoder_hidden_size, decoder_layers, dilations, drop_last_loader, early_stop_patience_steps, encoder_activation, encoder_hidden_size, futr_exog_list, h, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, kernel_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",37
neuralforecast.models.tft.GLU,GLU,class,Base class for all neural network modules.,"hidden_size, output_size",2
neuralforecast.models.tft.GRN,GRN,class,Base class for all neural network modules.,"activation, context_hidden_size, dropout, hidden_size, input_size, output_size",6
neuralforecast.models.tft.InterpretableMultiHeadAttention,InterpretableMultiHeadAttention,class,Base class for all neural network modules.,"attn_dropout, dropout, example_length, hidden_size, n_head",5
neuralforecast.models.tft.MaybeLayerNorm,MaybeLayerNorm,class,Base class for all neural network modules.,"eps, hidden_size, output_size",3
neuralforecast.models.tft.StaticCovariateEncoder,StaticCovariateEncoder,class,Base class for all neural network modules.,"dropout, grn_activation, hidden_size, n_rnn_layers, num_static_vars, one_rnn_initial_state, rnn_type",7
neuralforecast.models.tft.TFT,TFT,class,TFT,"alias, attn_dropout, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, futr_exog_list, grn_activation, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_head, n_rnn_layers, num_lr_decays, one_rnn_initial_state, optimizer, optimizer_kwargs, random_seed, rnn_type, scaler_type, start_padding_enabled, stat_exog_list, step_size, tgt_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",38
neuralforecast.models.tft.TFTEmbedding,TFTEmbedding,class,Base class for all neural network modules.,"futr_input_size, hidden_size, hist_input_size, stat_input_size, tgt_size",5
neuralforecast.models.tft.TemporalCovariateEncoder,TemporalCovariateEncoder,class,Base class for all neural network modules.,"dropout, grn_activation, hidden_size, n_rnn_layers, num_future_vars, num_historic_vars, rnn_type",7
neuralforecast.models.tft.TemporalFusionDecoder,TemporalFusionDecoder,class,Base class for all neural network modules.,"attn_dropout, dropout, encoder_length, example_length, grn_activation, hidden_size, n_head",7
neuralforecast.models.tft.VariableSelectionNetwork,VariableSelectionNetwork,class,Base class for all neural network modules.,"dropout, grn_activation, hidden_size, num_inputs",4
neuralforecast.models.tft.get_activation_fn,get_activation_fn,function,,activation_str,1
neuralforecast.models.tide.MLPResidual,MLPResidual,class,MLPResidual,"dropout, hidden_size, input_dim, layernorm, output_dim",5
neuralforecast.models.tide.TiDE,TiDE,class,TiDE,"alias, batch_size, dataloader_kwargs, decoder_output_dim, drop_last_loader, dropout, early_stop_patience_steps, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, layernorm, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_decoder_layers, num_encoder_layers, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, temporal_decoder_dim, temporal_width, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",38
neuralforecast.models.timellm.FlattenHead,FlattenHead,class,FlattenHead,"head_dropout, n_vars, nf, target_window",4
neuralforecast.models.timellm.PatchEmbedding,PatchEmbedding,class,PatchEmbedding,"d_model, dropout, patch_len, stride",4
neuralforecast.models.timellm.ReplicationPad1d,ReplicationPad1d,class,ReplicationPad1d,padding,1
neuralforecast.models.timellm.ReprogrammingLayer,ReprogrammingLayer,class,ReprogrammingLayer,"attention_dropout, d_keys, d_llm, d_model, n_heads",5
neuralforecast.models.timellm.TimeLLM,TimeLLM,class,TimeLLM,"alias, batch_size, d_ff, d_llm, d_model, dataloader_kwargs, dec_in, drop_last_loader, dropout, early_stop_patience_steps, enc_in, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, llm, llm_config, llm_num_hidden_layers, llm_output_attention, llm_output_hidden_states, llm_tokenizer, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_heads, num_lr_decays, optimizer, optimizer_kwargs, patch_len, prompt_prefix, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, stride, top_k, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",46
neuralforecast.models.timellm.TokenEmbedding,TokenEmbedding,class,TokenEmbedding,"c_in, d_model",2
neuralforecast.models.timemixer.DFT_series_decomp,DFT_series_decomp,class,Series decomposition block,top_k,1
neuralforecast.models.timemixer.DataEmbedding_wo_pos,DataEmbedding_wo_pos,class,DataEmbedding_wo_pos,"c_in, d_model, dropout, embed_type, freq",5
neuralforecast.models.timemixer.MultiScaleSeasonMixing,MultiScaleSeasonMixing,class,Bottom-up mixing season pattern,"down_sampling_layers, down_sampling_window, seq_len",3
neuralforecast.models.timemixer.MultiScaleTrendMixing,MultiScaleTrendMixing,class,Top-down mixing trend pattern,"down_sampling_layers, down_sampling_window, seq_len",3
neuralforecast.models.timemixer.PastDecomposableMixing,PastDecomposableMixing,class,PastDecomposableMixing,"channel_independence, d_ff, d_model, decomp_method, down_sampling_layers, down_sampling_window, dropout, moving_avg, pred_len, seq_len, top_k",11
neuralforecast.models.timemixer.TimeMixer,TimeMixer,class,TimeMixer,"alias, batch_size, channel_independence, d_ff, d_model, dataloader_kwargs, decoder_input_size_multiplier, decomp_method, down_sampling_layers, down_sampling_method, down_sampling_window, drop_last_loader, dropout, e_layers, early_stop_patience_steps, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, moving_avg, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, top_k, trainer_kwargs, training_data_availability_threshold, use_norm, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",43
neuralforecast.models.timesnet.FFT_for_Period,FFT_for_Period,function,,"k, x",2
neuralforecast.models.timesnet.Inception_Block_V1,Inception_Block_V1,class,Inception_Block_V1,"in_channels, init_weight, num_kernels, out_channels",4
neuralforecast.models.timesnet.TimesBlock,TimesBlock,class,TimesBlock,"conv_hidden_size, h, hidden_size, input_size, k, num_kernels",6
neuralforecast.models.timesnet.TimesNet,TimesNet,class,TimesNet,"alias, batch_size, conv_hidden_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, encoder_layers, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_kernels, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, top_k, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",36
neuralforecast.models.timexer.EnEmbedding,EnEmbedding,class,Base class for all neural network modules.,"d_model, dropout, n_vars, patch_len",4
neuralforecast.models.timexer.Encoder,Encoder,class,Base class for all neural network modules.,"layers, norm_layer, projection",3
neuralforecast.models.timexer.EncoderLayer,EncoderLayer,class,Base class for all neural network modules.,"activation, cross_attention, d_ff, d_model, dropout, self_attention",6
neuralforecast.models.timexer.FlattenHead,FlattenHead,class,Base class for all neural network modules.,"head_dropout, n_vars, nf, target_window",4
neuralforecast.models.timexer.TimeXer,TimeXer,class,TimeXer,"alias, batch_size, d_ff, dataloader_kwargs, drop_last_loader, dropout, e_layers, early_stop_patience_steps, exclude_insample_y, factor, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_heads, n_series, num_lr_decays, optimizer, optimizer_kwargs, patch_len, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, use_norm, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",39
neuralforecast.models.tsmixer.FeatureMixing,FeatureMixing,class,FeatureMixing,"dropout, ff_dim, input_size, n_series",4
neuralforecast.models.tsmixer.MixingLayer,MixingLayer,class,MixingLayer,"dropout, ff_dim, input_size, n_series",4
neuralforecast.models.tsmixer.TSMixer,TSMixer,class,TSMixer,"alias, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, exclude_insample_y, ff_dim, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_block, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, revin, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",35
neuralforecast.models.tsmixer.TemporalMixing,TemporalMixing,class,TemporalMixing,"dropout, input_size, n_series",3
neuralforecast.models.tsmixerx.FeatureMixing,FeatureMixing,class,FeatureMixing,"dropout, ff_dim, h, in_features, out_features",5
neuralforecast.models.tsmixerx.MixingLayer,MixingLayer,class,MixingLayer,"dropout, ff_dim, h, in_features, out_features",5
neuralforecast.models.tsmixerx.MixingLayerWithStaticExogenous,MixingLayerWithStaticExogenous,class,MixingLayerWithStaticExogenous,"dropout, ff_dim, h, stat_input_size",4
neuralforecast.models.tsmixerx.ReversibleInstanceNorm1d,ReversibleInstanceNorm1d,class,Base class for all neural network modules.,"eps, n_series",2
neuralforecast.models.tsmixerx.TSMixerx,TSMixerx,class,TSMixerx,"alias, batch_size, dataloader_kwargs, drop_last_loader, dropout, early_stop_patience_steps, exclude_insample_y, ff_dim, futr_exog_list, h, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_block, n_series, num_lr_decays, optimizer, optimizer_kwargs, random_seed, revin, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",35
neuralforecast.models.tsmixerx.TemporalMixing,TemporalMixing,class,TemporalMixing,"dropout, h, num_features",3
neuralforecast.models.vanillatransformer.VanillaTransformer,VanillaTransformer,class,VanillaTransformer,"activation, alias, batch_size, conv_hidden_size, dataloader_kwargs, decoder_input_size_multiplier, decoder_layers, drop_last_loader, dropout, early_stop_patience_steps, encoder_layers, exclude_insample_y, futr_exog_list, h, hidden_size, hist_exog_list, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, n_head, num_lr_decays, optimizer, optimizer_kwargs, random_seed, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",38
neuralforecast.models.xlstm.xLSTM,xLSTM,class,xLSTM,"alias, backbone, batch_size, dataloader_kwargs, decoder_activation, decoder_dropout, decoder_hidden_size, decoder_layers, drop_last_loader, early_stop_patience_steps, encoder_bias, encoder_dropout, encoder_hidden_size, encoder_n_blocks, exclude_insample_y, futr_exog_list, h, h_train, hist_exog_list, inference_input_size, inference_windows_batch_size, input_size, learning_rate, loss, lr_scheduler, lr_scheduler_kwargs, max_steps, num_lr_decays, optimizer, optimizer_kwargs, random_seed, recurrent, scaler_type, start_padding_enabled, stat_exog_list, step_size, trainer_kwargs, training_data_availability_threshold, val_check_steps, valid_batch_size, valid_loss, windows_batch_size",42
neuralforecast.tsdataset.BaseTimeSeriesDataset,BaseTimeSeriesDataset,class,Base class for time series datasets.,"max_size, min_size, static, static_cols, temporal_cols, y_idx",6
neuralforecast.tsdataset.LocalFilesTimeSeriesDataset,LocalFilesTimeSeriesDataset,class,Time series dataset that loads data from local files.,"files_ds, id_col, indices, last_times, max_size, min_size, static, static_cols, target_col, temporal_cols, time_col, y_idx",12
neuralforecast.tsdataset.TimeSeriesDataModule,TimeSeriesDataModule,class,PyTorch Lightning data module for time series datasets.,"batch_size, dataloaders_kwargs, dataset, drop_last, shuffle_train, valid_batch_size",6
neuralforecast.tsdataset.TimeSeriesDataset,TimeSeriesDataset,class,Time series dataset implementation.,"indptr, static, static_cols, temporal, temporal_cols, y_idx",6
neuralforecast.tsdataset.TimeSeriesLoader,TimeSeriesLoader,class,TimeSeriesLoader DataLoader.,"dataset, kwargs",2
neuralforecast.utils.DayOfMonth,DayOfMonth,class,"Day of month encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.DayOfWeek,DayOfWeek,class,"Day of week encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.DayOfYear,DayOfYear,class,"Day of year encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.HourOfDay,HourOfDay,class,"Hour of day encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.MinuteOfHour,MinuteOfHour,class,"Minute of hour encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.MonthOfYear,MonthOfYear,class,"Month of year encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.PredictionIntervals,PredictionIntervals,class,Class for storing prediction intervals metadata information.,"method, n_windows",2
neuralforecast.utils.SecondOfMinute,SecondOfMinute,class,"Second of minute encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.TimeFeature,TimeFeature,class,,,0
neuralforecast.utils.WeekOfYear,WeekOfYear,class,"Week of year encoded as value between [-0.5, 0.5].",,0
neuralforecast.utils.add_conformal_distribution_intervals,add_conformal_distribution_intervals,function,Add conformal intervals based on conformal scores using distribution strategy.,"cs_df, cs_n_windows, horizon, level, model, model_fcsts, n_series, quantiles",8
neuralforecast.utils.add_conformal_error_intervals,add_conformal_error_intervals,function,Add conformal intervals based on conformal scores using error strategy.,"cs_df, cs_n_windows, horizon, level, model, model_fcsts, n_series, quantiles",8
neuralforecast.utils.augment_calendar_df,augment_calendar_df,function,Augment a dataframe with calendar features based on frequency.,"df, freq",2
neuralforecast.utils.generate_series,generate_series,function,Generate Synthetic Panel Series.,"equal_ends, freq, max_length, min_length, n_series, n_static_features, n_temporal_features, seed",8
neuralforecast.utils.get_indexer_raise_missing,get_indexer_raise_missing,function,"Get index positions for values, raising error if any are missing.","idx, vals",2
neuralforecast.utils.get_prediction_interval_method,get_prediction_interval_method,function,Get the prediction interval method function by name.,method,1
neuralforecast.utils.level_to_quantiles,level_to_quantiles,function,Convert a list of confidence levels to quantiles.,level,1
neuralforecast.utils.quantiles_to_level,quantiles_to_level,function,Convert a list of quantiles to confidence levels.,quantiles,1
neuralforecast.utils.time_features_from_frequency_str,time_features_from_frequency_str,function,Returns a list of time features that will be appropriate for the given frequency string.,freq_str,1
