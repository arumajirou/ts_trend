Path,Type,Arguments,Return_Annotation,Summary (Docstring),Model_Name,Architecture,Future Exog,Historical Exog,Static Exog,Validation Support,Dropout,Scaler Support,Num_Arguments
neuralforecast.models.PatchTST,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, encoder_layers, n_heads, hidden_size, linear_hidden_size, dropout, fc_dropout, head_dropout, attn_dropout, patch_len, stride, revin, revin_affine, revin_subtract_last, activation, res_attention, batch_normalization, learn_pos_embed, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,PatchTST,PatchTST,Transformer-based,True,True,True,True,True,True,47
neuralforecast.models.TimeLLM,type,"h, input_size, patch_len, stride, d_ff, top_k, d_llm, d_model, n_heads, enc_in, dec_in, llm, llm_config, llm_tokenizer, llm_num_hidden_layers, llm_output_attention, llm_output_hidden_states, prompt_prefix, dropout, stat_exog_list, hist_exog_list, futr_exog_list, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeLLM,TimeLLM,Transformer-based,True,True,True,True,True,True,46
neuralforecast.models.TimeMixer,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, d_model, d_ff, dropout, e_layers, top_k, decomp_method, moving_avg, channel_independence, down_sampling_layers, down_sampling_window, down_sampling_method, use_norm, decoder_input_size_multiplier, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeMixer,TimeMixer,Other/Generic,True,True,True,True,True,True,43
neuralforecast.models.xLSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_blocks, encoder_hidden_size, encoder_bias, encoder_dropout, decoder_hidden_size, decoder_layers, decoder_dropout, decoder_activation, backbone, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,xLSTM,xLSTM,RNN-based,True,True,True,True,True,True,42
neuralforecast.models.GRU,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,GRU,GRU,RNN-based,True,True,True,True,True,True,41
neuralforecast.models.RNN,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,RNN,RNN,RNN-based,True,True,True,True,True,True,41
neuralforecast.models.FEDformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, decoder_input_size_multiplier, version, modes, mode_select, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,FEDformer,FEDformer,Transformer-based,True,True,True,True,True,True,41
neuralforecast.models.LSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,LSTM,LSTM,RNN-based,True,True,True,True,True,True,40
neuralforecast.models.Autoformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Autoformer,Autoformer,Transformer-based,True,True,True,True,True,True,40
neuralforecast.models.Informer,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, distil, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Informer,Informer,Transformer-based,True,True,True,True,True,True,40
neuralforecast.models.TimeXer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, patch_len, hidden_size, n_heads, e_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeXer,TimeXer,Other/Generic,True,True,True,True,True,True,39
neuralforecast.models.NHITS,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, stack_types, n_blocks, mlp_units, n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode, dropout_prob_theta, activation, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NHITS,NHITS,MLP/Linear-based,True,True,True,True,True,True,39
neuralforecast.models.iTransformer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, n_heads, e_layers, d_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,iTransformer,iTransformer,Transformer-based,True,True,True,True,True,True,39
neuralforecast.models.TFT,type,"h, input_size, tgt_size, stat_exog_list, hist_exog_list, futr_exog_list, hidden_size, n_head, attn_dropout, grn_activation, n_rnn_layers, rnn_type, one_rnn_initial_state, dropout, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TFT,TFT,Other/Generic,True,True,True,True,True,True,38
neuralforecast.models.TiDE,type,"h, input_size, hidden_size, decoder_output_dim, temporal_decoder_dim, dropout, layernorm, num_encoder_layers, num_decoder_layers, temporal_width, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TiDE,TiDE,MLP/Linear-based,True,True,True,True,True,True,38
neuralforecast.models.VanillaTransformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,VanillaTransformer,VanillaTransformer,Transformer-based,True,True,True,True,True,True,38
neuralforecast.models.NBEATSx,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_harmonics, n_polynomials, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATSx,NBEATSx,MLP/Linear-based,True,True,True,True,True,True,38
neuralforecast.models.KAN,type,"h, input_size, grid_size, spline_order, scale_noise, scale_base, scale_spline, enable_standalone_scale_spline, grid_eps, grid_range, n_hidden_layers, hidden_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, dataloader_kwargs, trainer_kwargs",,KAN,KAN,MLP/Linear-based,True,True,True,True,False,True,38
neuralforecast.models.DilatedRNN,type,"h, input_size, inference_input_size, cell_type, dilations, encoder_hidden_size, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DilatedRNN,DilatedRNN,RNN-based,True,True,True,True,False,True,37
neuralforecast.models.SOFTS,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, d_core, e_layers, d_ff, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,SOFTS,SOFTS,Other/Generic,True,True,True,True,True,True,37
neuralforecast.models.DeepAR,type,"h, input_size, h_train, lstm_n_layers, lstm_hidden_size, lstm_dropout, decoder_hidden_layers, decoder_hidden_size, trajectory_samples, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepAR,DeepAR,RNN-based,True,True,True,True,True,True,37
neuralforecast.models.TCN,type,"h, input_size, inference_input_size, kernel_size, dilations, encoder_hidden_size, encoder_activation, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TCN,TCN,CNN-based,True,True,True,True,False,True,37
neuralforecast.models.TimesNet,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, hidden_size, dropout, conv_hidden_size, top_k, num_kernels, encoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimesNet,TimesNet,Other/Generic,True,True,True,True,True,True,36
neuralforecast.models.NBEATS,type,"h, input_size, n_harmonics, n_polynomials, n_basis, basis, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATS,NBEATS,MLP/Linear-based,False,False,False,True,True,True,36
neuralforecast.models.TSMixerx,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixerx,TSMixerx,MLP/Linear-based,True,True,True,True,True,True,35
neuralforecast.models.TSMixer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixer,TSMixer,MLP/Linear-based,True,True,True,True,True,True,35
neuralforecast.models.StemGNN,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_stacks, multi_layer, dropout_rate, leaky_rate, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,StemGNN,StemGNN,Other/Generic,True,True,True,True,True,True,35
neuralforecast.models.RMoK,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, taylor_order, jacobi_degree, wavelet_function, dropout, revin_affine, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Reversible Mixture of KAN,RMoK,Other/Generic,True,True,True,True,True,True,35
neuralforecast.models.DeepNPTS,type,"h, input_size, hidden_size, batch_norm, dropout, n_layers, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepNPTS,DeepNPTS,Other/Generic,True,True,True,True,True,True,34
neuralforecast.models.autoformer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.,BaseModel,Other/Generic,True,True,True,True,False,True,34
neuralforecast.models.patchtst.PatchTST_backbone,type,"c_in, c_out, input_size, h, patch_len, stride, max_seq_len, n_layers, hidden_size, n_heads, d_k, d_v, linear_hidden_size, norm, attn_dropout, dropout, act, key_padding_mask, padding_var, attn_mask, res_attention, pre_norm, store_attn, pe, learn_pe, fc_dropout, head_dropout, padding_patch, pretrain_head, head_type, individual, revin, affine, subtract_last",,PatchTST_backbone,PatchTST_backbone,Transformer-based,False,False,False,False,True,False,34
neuralforecast.models.MLPMultivariate,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLPMultivariate,MLPMultivariate,MLP/Linear-based,True,True,True,True,False,True,33
neuralforecast.models.MLP,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLP,MLP,MLP/Linear-based,True,True,True,True,False,True,32
neuralforecast.models.BiTCN,type,"h, input_size, hidden_size, dropout, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,BiTCN,BiTCN,CNN-based,True,True,True,True,True,True,32
neuralforecast.models.DLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, moving_avg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DLinear,DLinear,MLP/Linear-based,True,True,True,True,False,True,31
neuralforecast.models.NLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NLinear,NLinear,MLP/Linear-based,True,True,True,True,False,True,30
neuralforecast.models.patchtst.TSTiEncoder,type,"c_in, patch_num, patch_len, max_seq_len, n_layers, hidden_size, n_heads, d_k, d_v, linear_hidden_size, norm, attn_dropout, dropout, act, store_attn, key_padding_mask, padding_var, attn_mask, res_attention, pre_norm, pe, learn_pe",,TSTiEncoder,TSTiEncoder,Transformer-based,False,False,False,False,True,False,22
neuralforecast.models.patchtst.TSTEncoder,type,"q_len, hidden_size, n_heads, d_k, d_v, linear_hidden_size, norm, attn_dropout, dropout, activation, res_attention, n_layers, pre_norm, store_attn",,TSTEncoder,TSTEncoder,Transformer-based,False,False,False,False,True,False,14
neuralforecast.models.patchtst.TSTEncoderLayer,type,"q_len, hidden_size, n_heads, d_k, d_v, linear_hidden_size, store_attn, norm, attn_dropout, dropout, bias, activation, res_attention, pre_norm",,TSTEncoderLayer,TSTEncoderLayer,Transformer-based,False,False,False,False,True,False,14
neuralforecast.models.nhits.NHITSBlock,type,"input_size, h, n_theta, mlp_units, basis, futr_input_size, hist_input_size, stat_input_size, n_pool_kernel_size, pooling_mode, dropout_prob, activation",,NHITS block which takes a basis function as an argument.,NHITSBlock,MLP/Linear-based,False,False,False,False,True,False,12
neuralforecast.models.kan.KANLinear,type,"in_features, out_features, grid_size, spline_order, scale_noise, scale_base, scale_spline, enable_standalone_scale_spline, base_activation, grid_eps, grid_range",,KANLinear,KANLinear,MLP/Linear-based,False,False,False,False,False,False,11
neuralforecast.models.timemixer.PastDecomposableMixing,type,"seq_len, pred_len, down_sampling_window, down_sampling_layers, d_model, dropout, channel_independence, decomp_method, d_ff, moving_avg, top_k",,PastDecomposableMixing,PastDecomposableMixing,Other/Generic,False,False,False,False,True,False,11
neuralforecast.models.autoformer.DecoderLayer,type,"self_attention, cross_attention, hidden_size, c_out, conv_hidden_size, MovingAvg, dropout, activation",,Autoformer decoder layer with the progressive decomposition architecture,DecoderLayer,Other/Generic,False,False,False,False,True,False,8
neuralforecast.models.bitcn.TCNCell,type,"in_channels, out_channels, kernel_size, padding, dilation, mode, groups, dropout",,"Temporal Convolutional Network Cell, consisting of CustomConv1D modules.",TCNCell,CNN-based,False,False,False,False,True,False,8
neuralforecast.models.fedformer.FourierCrossAttention,type,"in_channels, out_channels, seq_len_q, seq_len_kv, modes, mode_select_method, activation, policy",,Fourier Cross Attention layer,FourierCrossAttention,Transformer-based,False,False,False,False,False,False,8
neuralforecast.models.tft.StaticCovariateEncoder,type,"hidden_size, num_static_vars, dropout, grn_activation, rnn_type, n_rnn_layers, one_rnn_initial_state",,Base class for all neural network modules.,StaticCovariateEncoder,Other/Generic,False,False,False,False,True,False,7
neuralforecast.models.tft.TemporalCovariateEncoder,type,"hidden_size, num_historic_vars, num_future_vars, dropout, grn_activation, rnn_type, n_rnn_layers",,Base class for all neural network modules.,TemporalCovariateEncoder,Other/Generic,False,False,False,False,True,False,7
neuralforecast.models.dilated_rnn.DRNN,type,"n_input, n_hidden, n_layers, dilations, dropout, cell_type, batch_first",,Base class for all neural network modules.,DRNN,RNN-based,False,False,False,False,True,False,7
neuralforecast.models.deepar.DistributionLoss,type,"distribution, level, quantiles, num_samples, return_params, horizon_weight, distribution_kwargs",,DistributionLoss,DistributionLoss,Other/Generic,False,False,False,False,False,False,7
neuralforecast.models.tft.TemporalFusionDecoder,type,"n_head, hidden_size, example_length, encoder_length, attn_dropout, dropout, grn_activation",,Base class for all neural network modules.,TemporalFusionDecoder,Other/Generic,False,False,False,False,True,False,7
neuralforecast.models.bitcn.CustomConv1d,type,"in_channels, out_channels, kernel_size, padding, dilation, mode, groups",,Forward- and backward looking Conv1D,CustomConv1d,Other/Generic,False,False,False,False,False,False,7
neuralforecast.models.autoformer.EncoderLayer,type,"attention, hidden_size, conv_hidden_size, MovingAvg, dropout, activation",,Autoformer encoder layer with the progressive decomposition architecture,EncoderLayer,Other/Generic,False,False,False,False,True,False,6
neuralforecast.models.nbeats.NBEATSBlock,type,"input_size, n_theta, mlp_units, basis, dropout_prob, activation",,N-BEATS block which takes a basis function as an argument.,NBEATSBlock,MLP/Linear-based,False,False,False,False,True,False,6
neuralforecast.models.patchtst.Flatten_Head,type,"individual, n_vars, nf, h, c_out, head_dropout",,Flatten_Head,Flatten_Head,Other/Generic,False,False,False,False,True,False,6
neuralforecast.models.timesnet.TimesBlock,type,"input_size, h, k, hidden_size, conv_hidden_size, num_kernels",,TimesBlock,TimesBlock,Other/Generic,False,False,False,False,False,False,6
neuralforecast.models.tft.GRN,type,"input_size, hidden_size, output_size, context_hidden_size, dropout, activation",,Base class for all neural network modules.,GRN,Other/Generic,False,False,False,False,True,False,6
neuralforecast.models.informer.TransDecoderLayer,type,"self_attention, cross_attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.,TransDecoderLayer,Other/Generic,False,False,False,False,True,False,6
neuralforecast.models.informer.ProbMask,type,"B, H, L, index, scores, device",,ProbMask,ProbMask,Other/Generic,False,False,False,False,False,False,6
neuralforecast.models.patchtst.RevIN,type,"num_features, eps, affine, subtract_last, non_norm",,RevIN (Reversible-Instance-Normalization),RevIN,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.tide.MLPResidual,type,"input_dim, hidden_size, output_dim, dropout, layernorm",,MLPResidual,MLPResidual,MLP/Linear-based,False,False,False,False,True,False,5
neuralforecast.models.tft.InterpretableMultiHeadAttention,type,"n_head, hidden_size, example_length, attn_dropout, dropout",,Base class for all neural network modules.,InterpretableMultiHeadAttention,Transformer-based,False,False,False,False,True,False,5
neuralforecast.models.rmok.RevINMultivariate,type,"num_features, eps, affine, subtract_last, non_norm",,ReversibleInstanceNorm1d for Multivariate models,RevINMultivariate,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.tft.TFTEmbedding,type,"hidden_size, stat_input_size, futr_input_size, hist_input_size, tgt_size",,Base class for all neural network modules.,TFTEmbedding,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.rmok.JacobiKANLayer,type,"input_dim, output_dim, degree, a, b",,https://github.com/SpaceLearner/JacobiKAN/blob/main/JacobiKANLayer.py,JacobiKANLayer,MLP/Linear-based,False,False,False,False,False,False,5
neuralforecast.models.autoformer.DataEmbedding,type,"c_in, exog_input_size, hidden_size, pos_embedding, dropout",,Base class for all neural network modules.,DataEmbedding,Other/Generic,False,False,False,False,True,False,5
neuralforecast.models.fedformer.FourierBlock,type,"in_channels, out_channels, seq_len, modes, mode_select_method",,Fourier block,FourierBlock,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.autoformer.AutoCorrelation,type,"mask_flag, factor, scale, attention_dropout, output_attention",,AutoCorrelation Mechanism with the following two phases:,AutoCorrelation,Other/Generic,False,False,False,False,True,False,5
neuralforecast.models.autoformer.AutoCorrelationLayer,type,"correlation, hidden_size, n_head, d_keys, d_values",,Auto Correlation Layer,AutoCorrelationLayer,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.HINT,type,"h, S, model, reconciliation, alias",,HINT,HINT,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.timellm.ReprogrammingLayer,type,"d_model, n_heads, d_keys, d_llm, attention_dropout",,ReprogrammingLayer,ReprogrammingLayer,Other/Generic,False,False,False,False,True,False,5
neuralforecast.models.timemixer.DataEmbedding_wo_pos,type,"c_in, d_model, dropout, embed_type, freq",,DataEmbedding_wo_pos,DataEmbedding_wo_pos,Other/Generic,False,False,False,False,True,False,5
neuralforecast.models.tcn.TemporalConvolutionEncoder,type,"in_channels, out_channels, kernel_size, dilations, activation",,Temporal Convolution Encoder,TemporalConvolutionEncoder,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.rmok.WaveKANLayer,type,"in_features, out_features, wavelet_type, with_bn, device",,This is a sample code for the simulations of the paper:,WaveKANLayer,MLP/Linear-based,False,False,False,False,False,False,5
neuralforecast.models.nbeats.TrendBasis,type,"n_basis, backcast_size, forecast_size, out_features, basis",,Base class for all neural network modules.,TrendBasis,Other/Generic,False,False,False,False,False,False,5
neuralforecast.models.informer.TransEncoderLayer,type,"attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.,TransEncoderLayer,Other/Generic,False,False,False,False,True,False,5
neuralforecast.models.itransformer.FullAttention,type,"mask_flag, factor, scale, attention_dropout, output_attention",,Base class for all neural network modules.,FullAttention,Transformer-based,False,False,False,False,True,False,5
neuralforecast.models.informer.ProbAttention,type,"mask_flag, factor, scale, attention_dropout, output_attention",,ProbAttention,ProbAttention,Transformer-based,False,False,False,False,True,False,5
neuralforecast.models.informer.AttentionLayer,type,"attention, hidden_size, n_heads, d_keys, d_values",,Base class for all neural network modules.,AttentionLayer,Transformer-based,False,False,False,False,False,False,5
neuralforecast.models.timexer.EnEmbedding,type,"n_vars, d_model, patch_len, dropout",,Base class for all neural network modules.,EnEmbedding,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.rmok.TaylorKANLayer,type,"input_dim, out_dim, order, addbias",,https://github.com/Muyuzhierchengse/TaylorKAN/,TaylorKANLayer,MLP/Linear-based,False,False,False,False,False,False,4
neuralforecast.models.timesnet.Inception_Block_V1,type,"in_channels, out_channels, num_kernels, init_weight",,Inception_Block_V1,Inception_Block_V1,Other/Generic,False,False,False,False,False,False,4
neuralforecast.models.tsmixer.FeatureMixing,type,"n_series, input_size, dropout, ff_dim",,FeatureMixing,FeatureMixing,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.tsmixerx.MixingLayerWithStaticExogenous,type,"h, dropout, ff_dim, stat_input_size",,MixingLayerWithStaticExogenous,MixingLayerWithStaticExogenous,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.tsmixer.MixingLayer,type,"n_series, input_size, dropout, ff_dim",,MixingLayer,MixingLayer,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.timellm.FlattenHead,type,"n_vars, nf, target_window, head_dropout",,FlattenHead,FlattenHead,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.tft.VariableSelectionNetwork,type,"hidden_size, num_inputs, dropout, grn_activation",,Base class for all neural network modules.,VariableSelectionNetwork,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.stemgnn.StockBlockLayer,type,"time_step, unit, multi_layer, stack_cnt",,StockBlockLayer,StockBlockLayer,Other/Generic,False,False,False,False,False,False,4
neuralforecast.models.timellm.PatchEmbedding,type,"d_model, patch_len, stride, dropout",,PatchEmbedding,PatchEmbedding,Other/Generic,False,False,False,False,True,False,4
neuralforecast.models.nbeats.SeasonalityBasis,type,"harmonics, backcast_size, forecast_size, out_features",,Base class for all neural network modules.,SeasonalityBasis,Other/Generic,False,False,False,False,False,False,4
neuralforecast.models.timemixer.TemporalEmbedding,type,"d_model, embed_type, freq",,Base class for all neural network modules.,TemporalEmbedding,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.tsmixer.TemporalMixing,type,"n_series, input_size, dropout",,TemporalMixing,TemporalMixing,Other/Generic,False,False,False,False,True,False,3
neuralforecast.models.dilated_rnn.ResLSTMLayer,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.,ResLSTMLayer,RNN-based,False,False,False,False,True,False,3
neuralforecast.models.dilated_rnn.ResLSTMCell,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.,ResLSTMCell,RNN-based,False,False,False,False,True,False,3
neuralforecast.models.dilated_rnn.LSTMCell,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.,LSTMCell,RNN-based,False,False,False,False,True,False,3
neuralforecast.models.autoformer.Encoder,type,"attn_layers, conv_layers, norm_layer",,Autoformer encoder,Encoder,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.autoformer.Decoder,type,"layers, norm_layer, projection",,Autoformer decoder,Decoder,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.timemixer.MultiScaleTrendMixing,type,"seq_len, down_sampling_window, down_sampling_layers",,Top-down mixing trend pattern,MultiScaleTrendMixing,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.timemixer.MultiScaleSeasonMixing,type,"seq_len, down_sampling_window, down_sampling_layers",,Bottom-up mixing season pattern,MultiScaleSeasonMixing,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.dilated_rnn.AttentiveLSTMLayer,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.,AttentiveLSTMLayer,RNN-based,False,False,False,False,True,False,3
neuralforecast.models.informer.TransDecoder,type,"layers, norm_layer, projection",,Base class for all neural network modules.,TransDecoder,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.nbeats.IdentityBasis,type,"backcast_size, forecast_size, out_features",,Base class for all neural network modules.,IdentityBasis,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.informer.TransEncoder,type,"attn_layers, conv_layers, norm_layer",,Base class for all neural network modules.,TransEncoder,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.tft.MaybeLayerNorm,type,"output_size, hidden_size, eps",,Base class for all neural network modules.,MaybeLayerNorm,Other/Generic,False,False,False,False,False,False,3
neuralforecast.models.itransformer.DataEmbedding_inverted,type,"c_in, hidden_size, dropout",,DataEmbedding_inverted,DataEmbedding_inverted,Other/Generic,False,False,False,False,True,False,3
neuralforecast.models.patchtst.Transpose,type,"dims, contiguous",,Transpose,Transpose,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.dlinear.MovingAvg,type,"kernel_size, stride",,Moving average block to highlight the trend of time series,MovingAvg,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.tsmixerx.ReversibleInstanceNorm1d,type,"n_series, eps",,Base class for all neural network modules.,ReversibleInstanceNorm1d,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.timemixer.PositionalEmbedding,type,"hidden_size, max_len",,Base class for all neural network modules.,PositionalEmbedding,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.softs.STAD,type,"d_series, d_core",,STar Aggregate Dispatch Module,STAD,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.stemgnn.GLU,type,"input_channel, output_channel",,GLU,GLU,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.timellm.TokenEmbedding,type,"c_in, d_model",,TokenEmbedding,TokenEmbedding,Other/Generic,False,False,False,False,False,False,2
neuralforecast.models.autoformer.SeriesDecomp,type,kernel_size,,Series decomposition block,SeriesDecomp,Other/Generic,False,False,False,False,False,False,1
neuralforecast.models.autoformer.MAE,type,horizon_weight,,Mean Absolute Error.,MAE,Other/Generic,False,False,False,False,False,False,1
neuralforecast.models.autoformer.LayerNorm,type,channels,,Special designed layernorm for the seasonal part,LayerNorm,Other/Generic,False,False,False,False,False,False,1
neuralforecast.models.nbeatsx.ExogenousBasis,type,forecast_size,,Base class for all neural network modules.,ExogenousBasis,Other/Generic,False,False,False,False,False,False,1
neuralforecast.models.informer.ConvLayer,type,c_in,,ConvLayer,ConvLayer,Other/Generic,False,False,False,False,False,False,1
neuralforecast.models.timellm.ReplicationPad1d,type,padding,,ReplicationPad1d,ReplicationPad1d,Other/Generic,False,False,False,False,False,False,1
neuralforecast.models.timemixer.DFT_series_decomp,type,top_k,,Series decomposition block,DFT_series_decomp,Other/Generic,False,False,False,False,False,False,1
