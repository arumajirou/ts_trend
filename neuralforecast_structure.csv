Path,Type,Arguments,Return_Annotation,Summary (Docstring)
neuralforecast.DistributedConfig,type,"partitions_path, num_nodes, devices",None,"DistributedConfig(partitions_path: str, num_nodes: int, devices: int)"
neuralforecast.NeuralForecast,type,"models, freq, local_scaler_type",,
neuralforecast.common.enums.ExplainerEnum,EnumType,"args, kwds",,str(object='') -> str
neuralforecast.common.enums.TimeSeriesDatasetEnum,EnumType,"args, kwds",,str(object='') -> str
neuralforecast.compat.SparkDataFrame,type,"args, kwargs",,
neuralforecast.core.Autoformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Autoformer
neuralforecast.core.BaseAuto,type,"cls_model, h, loss, valid_loss, config, search_alg, num_samples, cpus, gpus, refit_with_val, verbose, alias, backend, callbacks",,"Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to"
neuralforecast.core.BiTCN,type,"h, input_size, hidden_size, dropout, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,BiTCN
neuralforecast.core.DLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, moving_avg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DLinear
neuralforecast.core.DeepAR,type,"h, input_size, h_train, lstm_n_layers, lstm_hidden_size, lstm_dropout, decoder_hidden_layers, decoder_hidden_size, trajectory_samples, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepAR
neuralforecast.core.DeepNPTS,type,"h, input_size, hidden_size, batch_norm, dropout, n_layers, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepNPTS
neuralforecast.core.DilatedRNN,type,"h, input_size, inference_input_size, cell_type, dilations, encoder_hidden_size, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DilatedRNN
neuralforecast.core.DistributedConfig,type,"partitions_path, num_nodes, devices",None,"DistributedConfig(partitions_path: str, num_nodes: int, devices: int)"
neuralforecast.core.ExplainerEnum,EnumType,"args, kwds",,str(object='') -> str
neuralforecast.core.FEDformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, decoder_input_size_multiplier, version, modes, mode_select, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,FEDformer
neuralforecast.core.GRU,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,GRU
neuralforecast.core.HuberIQLoss,type,"cos_embedding_dim, concentration0, concentration1, delta, horizon_weight",,Implicit Huber Quantile Loss
neuralforecast.core.IQLoss,type,"cos_embedding_dim, concentration0, concentration1, horizon_weight",,Implicit Quantile Loss.
neuralforecast.core.Informer,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, distil, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Informer
neuralforecast.core.KAN,type,"h, input_size, grid_size, spline_order, scale_noise, scale_base, scale_spline, enable_standalone_scale_spline, grid_eps, grid_range, n_hidden_layers, hidden_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, dataloader_kwargs, trainer_kwargs",,KAN
neuralforecast.core.LSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,LSTM
neuralforecast.core.LocalFilesTimeSeriesDataset,type,"files_ds, temporal_cols, id_col, time_col, target_col, last_times, indices, max_size, min_size, y_idx, static, static_cols",,Time series dataset that loads data from local files.
neuralforecast.core.MLP,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLP
neuralforecast.core.MLPMultivariate,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLPMultivariate
neuralforecast.core.MockTrial,type,"args, kwargs",,
neuralforecast.core.NBEATS,type,"h, input_size, n_harmonics, n_polynomials, n_basis, basis, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATS
neuralforecast.core.NBEATSx,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_harmonics, n_polynomials, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATSx
neuralforecast.core.NHITS,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, stack_types, n_blocks, mlp_units, n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode, dropout_prob_theta, activation, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NHITS
neuralforecast.core.NLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NLinear
neuralforecast.core.NeuralForecast,type,"models, freq, local_scaler_type",,
neuralforecast.core.PatchTST,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, encoder_layers, n_heads, hidden_size, linear_hidden_size, dropout, fc_dropout, head_dropout, attn_dropout, patch_len, stride, revin, revin_affine, revin_subtract_last, activation, res_attention, batch_normalization, learn_pos_embed, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,PatchTST
neuralforecast.core.PredictionIntervals,type,"n_windows, method",,Class for storing prediction intervals metadata information.
neuralforecast.core.RMoK,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, taylor_order, jacobi_degree, wavelet_function, dropout, revin_affine, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Reversible Mixture of KAN
neuralforecast.core.RNN,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,RNN
neuralforecast.core.SOFTS,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, d_core, e_layers, d_ff, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,SOFTS
neuralforecast.core.SparkDataFrame,type,"args, kwargs",,
neuralforecast.core.StemGNN,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_stacks, multi_layer, dropout_rate, leaky_rate, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,StemGNN
neuralforecast.core.TCN,type,"h, input_size, inference_input_size, kernel_size, dilations, encoder_hidden_size, encoder_activation, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TCN
neuralforecast.core.TFT,type,"h, input_size, tgt_size, stat_exog_list, hist_exog_list, futr_exog_list, hidden_size, n_head, attn_dropout, grn_activation, n_rnn_layers, rnn_type, one_rnn_initial_state, dropout, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TFT
neuralforecast.core.TSMixer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixer
neuralforecast.core.TSMixerx,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixerx
neuralforecast.core.TiDE,type,"h, input_size, hidden_size, decoder_output_dim, temporal_decoder_dim, dropout, layernorm, num_encoder_layers, num_decoder_layers, temporal_width, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TiDE
neuralforecast.core.TimeLLM,type,"h, input_size, patch_len, stride, d_ff, top_k, d_llm, d_model, n_heads, enc_in, dec_in, llm, llm_config, llm_tokenizer, llm_num_hidden_layers, llm_output_attention, llm_output_hidden_states, prompt_prefix, dropout, stat_exog_list, hist_exog_list, futr_exog_list, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeLLM
neuralforecast.core.TimeMixer,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, d_model, d_ff, dropout, e_layers, top_k, decomp_method, moving_avg, channel_independence, down_sampling_layers, down_sampling_window, down_sampling_method, use_norm, decoder_input_size_multiplier, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeMixer
neuralforecast.core.TimeSeriesDataset,type,"temporal, temporal_cols, indptr, y_idx, static, static_cols",,Time series dataset implementation.
neuralforecast.core.TimeXer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, patch_len, hidden_size, n_heads, e_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeXer
neuralforecast.core.TimesNet,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, hidden_size, dropout, conv_hidden_size, top_k, num_kernels, encoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimesNet
neuralforecast.core.VanillaTransformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,VanillaTransformer
neuralforecast.core.fsspec.exceptions.asyncio.base_events.coroutines.inspect.isgenerator,function,obj,,
neuralforecast.core.get_prediction_interval_method,function,method,,Get the prediction interval method function by name.
neuralforecast.core.iTransformer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, n_heads, e_layers, d_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,iTransformer
neuralforecast.core.level_to_quantiles,function,level,typing.List[float],Convert a list of confidence levels to quantiles.
neuralforecast.core.quantiles_to_level,function,quantiles,"typing.List[typing.Union[int, float]]",Convert a list of quantiles to confidence levels.
neuralforecast.core.xLSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_blocks, encoder_hidden_size, encoder_bias, encoder_dropout, decoder_hidden_size, decoder_layers, decoder_dropout, decoder_activation, backbone, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,xLSTM
neuralforecast.losses.pytorch.Accuracy,type,,,Accuracy
neuralforecast.losses.pytorch.BaseISQF,type,"spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x, tol, validate_args",None,Base distribution class for the Incremental (Spline) Quantile Function.
neuralforecast.losses.pytorch.BasePointLoss,type,"horizon_weight, outputsize_multiplier, output_names",,Base class for point loss functions.
neuralforecast.losses.pytorch.DistributionLoss,type,"distribution, level, quantiles, num_samples, return_params, horizon_weight, distribution_kwargs",,DistributionLoss
neuralforecast.losses.pytorch.GMM,type,"n_components, level, quantiles, num_samples, return_params, batch_correlation, horizon_correlation, weighted",,Gaussian Mixture Mesh
neuralforecast.losses.pytorch.HuberIQLoss,type,"cos_embedding_dim, concentration0, concentration1, delta, horizon_weight",,Implicit Huber Quantile Loss
neuralforecast.losses.pytorch.HuberLoss,type,"delta, horizon_weight",,Huber Loss
neuralforecast.losses.pytorch.HuberMQLoss,type,"level, quantiles, delta, horizon_weight",,Huberized Multi-Quantile loss
neuralforecast.losses.pytorch.HuberQLoss,type,"q, delta, horizon_weight",,Huberized Quantile Loss
neuralforecast.losses.pytorch.IQLoss,type,"cos_embedding_dim, concentration0, concentration1, horizon_weight",,Implicit Quantile Loss.
neuralforecast.losses.pytorch.ISQF,type,"spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x, loc, scale, validate_args",None,Distribution class for the Incremental (Spline) Quantile Function.
neuralforecast.losses.pytorch.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.losses.pytorch.MAPE,type,horizon_weight,,Mean Absolute Percentage Error
neuralforecast.losses.pytorch.MASE,type,"seasonality, horizon_weight",,Mean Absolute Scaled Error
neuralforecast.losses.pytorch.MQLoss,type,"level, quantiles, horizon_weight",,Multi-Quantile loss
neuralforecast.losses.pytorch.MSE,type,horizon_weight,,Mean Squared Error.
neuralforecast.losses.pytorch.NBMM,type,"n_components, level, quantiles, num_samples, return_params, weighted",,Negative Binomial Mixture Mesh
neuralforecast.losses.pytorch.PMM,type,"n_components, level, quantiles, num_samples, return_params, batch_correlation, horizon_correlation, weighted",,Poisson Mixture Mesh
neuralforecast.losses.pytorch.QuantileLayer,type,"num_output, cos_embedding_dim",,Implicit Quantile Layer from the paper IQN for Distributional Reinforcement Learning.
neuralforecast.losses.pytorch.QuantileLoss,type,"q, horizon_weight",,Quantile Loss.
neuralforecast.losses.pytorch.RMSE,type,horizon_weight,,Root Mean Squared Error.
neuralforecast.losses.pytorch.SMAPE,type,horizon_weight,,Symmetric Mean Absolute Percentage Error
neuralforecast.losses.pytorch.TukeyLoss,type,"c, normalize",,Tukey Loss
neuralforecast.losses.pytorch.Tweedie,type,"log_mu, rho, validate_args",,Tweedie Distribution.
neuralforecast.losses.pytorch.bernoulli_scale_decouple,function,"output, loc, scale",,Bernoulli Scale Decouple.
neuralforecast.losses.pytorch.est_alpha,function,rho,,
neuralforecast.losses.pytorch.est_beta,function,"mu, rho",,
neuralforecast.losses.pytorch.est_lambda,function,"mu, rho",,
neuralforecast.losses.pytorch.isqf_domain_map,function,"input, tol, quantiles, num_pieces",,ISQF Domain Map
neuralforecast.losses.pytorch.isqf_scale_decouple,function,"output, loc, scale",,ISQF Scale Decouple
neuralforecast.losses.pytorch.level_to_outputs,function,level,,
neuralforecast.losses.pytorch.nbinomial_scale_decouple,function,"output, loc, scale",,Negative Binomial Scale Decouple
neuralforecast.losses.pytorch.normal_scale_decouple,function,"output, loc, scale, eps",,Normal Scale Decouple.
neuralforecast.losses.pytorch.poisson_scale_decouple,function,"output, loc, scale",,Poisson Scale Decouple
neuralforecast.losses.pytorch.quantiles_to_outputs,function,quantiles,,
neuralforecast.losses.pytorch.relMSE,type,"y_train, horizon_weight",,Relative Mean Squared Error
neuralforecast.losses.pytorch.sCRPS,type,"level, quantiles",,Scaled Continues Ranked Probability Score
neuralforecast.losses.pytorch.student_scale_decouple,function,"output, loc, scale, eps",,Student-T Scale Decouple.
neuralforecast.losses.pytorch.tweedie_domain_map,function,"input, rho",,Maps output of neural network to domain of distribution loss
neuralforecast.losses.pytorch.tweedie_scale_decouple,function,"output, loc, scale",,Tweedie Scale Decouple
neuralforecast.losses.pytorch.weighted_average,function,"x, weights, dim",<class 'torch.Tensor'>,Computes the weighted average of a given tensor across a given dim.
neuralforecast.models.Autoformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Autoformer
neuralforecast.models.BiTCN,type,"h, input_size, hidden_size, dropout, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,BiTCN
neuralforecast.models.DLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, moving_avg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DLinear
neuralforecast.models.DeepAR,type,"h, input_size, h_train, lstm_n_layers, lstm_hidden_size, lstm_dropout, decoder_hidden_layers, decoder_hidden_size, trajectory_samples, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepAR
neuralforecast.models.DeepNPTS,type,"h, input_size, hidden_size, batch_norm, dropout, n_layers, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepNPTS
neuralforecast.models.DilatedRNN,type,"h, input_size, inference_input_size, cell_type, dilations, encoder_hidden_size, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DilatedRNN
neuralforecast.models.FEDformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, decoder_input_size_multiplier, version, modes, mode_select, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,FEDformer
neuralforecast.models.GRU,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,GRU
neuralforecast.models.HINT,type,"h, S, model, reconciliation, alias",,HINT
neuralforecast.models.Informer,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, distil, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Informer
neuralforecast.models.KAN,type,"h, input_size, grid_size, spline_order, scale_noise, scale_base, scale_spline, enable_standalone_scale_spline, grid_eps, grid_range, n_hidden_layers, hidden_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, dataloader_kwargs, trainer_kwargs",,KAN
neuralforecast.models.LSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,LSTM
neuralforecast.models.MLP,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLP
neuralforecast.models.MLPMultivariate,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLPMultivariate
neuralforecast.models.NBEATS,type,"h, input_size, n_harmonics, n_polynomials, n_basis, basis, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATS
neuralforecast.models.NBEATSx,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_harmonics, n_polynomials, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATSx
neuralforecast.models.NHITS,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, stack_types, n_blocks, mlp_units, n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode, dropout_prob_theta, activation, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NHITS
neuralforecast.models.NLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NLinear
neuralforecast.models.PatchTST,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, encoder_layers, n_heads, hidden_size, linear_hidden_size, dropout, fc_dropout, head_dropout, attn_dropout, patch_len, stride, revin, revin_affine, revin_subtract_last, activation, res_attention, batch_normalization, learn_pos_embed, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,PatchTST
neuralforecast.models.RMoK,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, taylor_order, jacobi_degree, wavelet_function, dropout, revin_affine, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Reversible Mixture of KAN
neuralforecast.models.RNN,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,RNN
neuralforecast.models.SOFTS,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, d_core, e_layers, d_ff, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,SOFTS
neuralforecast.models.StemGNN,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_stacks, multi_layer, dropout_rate, leaky_rate, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,StemGNN
neuralforecast.models.TCN,type,"h, input_size, inference_input_size, kernel_size, dilations, encoder_hidden_size, encoder_activation, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TCN
neuralforecast.models.TFT,type,"h, input_size, tgt_size, stat_exog_list, hist_exog_list, futr_exog_list, hidden_size, n_head, attn_dropout, grn_activation, n_rnn_layers, rnn_type, one_rnn_initial_state, dropout, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TFT
neuralforecast.models.TSMixer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixer
neuralforecast.models.TSMixerx,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixerx
neuralforecast.models.TiDE,type,"h, input_size, hidden_size, decoder_output_dim, temporal_decoder_dim, dropout, layernorm, num_encoder_layers, num_decoder_layers, temporal_width, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TiDE
neuralforecast.models.TimeLLM,type,"h, input_size, patch_len, stride, d_ff, top_k, d_llm, d_model, n_heads, enc_in, dec_in, llm, llm_config, llm_tokenizer, llm_num_hidden_layers, llm_output_attention, llm_output_hidden_states, prompt_prefix, dropout, stat_exog_list, hist_exog_list, futr_exog_list, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeLLM
neuralforecast.models.TimeMixer,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, d_model, d_ff, dropout, e_layers, top_k, decomp_method, moving_avg, channel_independence, down_sampling_layers, down_sampling_window, down_sampling_method, use_norm, decoder_input_size_multiplier, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeMixer
neuralforecast.models.TimeXer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, patch_len, hidden_size, n_heads, e_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeXer
neuralforecast.models.TimesNet,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, hidden_size, dropout, conv_hidden_size, top_k, num_kernels, encoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimesNet
neuralforecast.models.VanillaTransformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,VanillaTransformer
neuralforecast.models.autoformer.AutoCorrelation,type,"mask_flag, factor, scale, attention_dropout, output_attention",,AutoCorrelation Mechanism with the following two phases:
neuralforecast.models.autoformer.AutoCorrelationLayer,type,"correlation, hidden_size, n_head, d_keys, d_values",,Auto Correlation Layer
neuralforecast.models.autoformer.Autoformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Autoformer
neuralforecast.models.autoformer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.autoformer.DataEmbedding,type,"c_in, exog_input_size, hidden_size, pos_embedding, dropout",,Base class for all neural network modules.
neuralforecast.models.autoformer.Decoder,type,"layers, norm_layer, projection",,Autoformer decoder
neuralforecast.models.autoformer.DecoderLayer,type,"self_attention, cross_attention, hidden_size, c_out, conv_hidden_size, MovingAvg, dropout, activation",,Autoformer decoder layer with the progressive decomposition architecture
neuralforecast.models.autoformer.Encoder,type,"attn_layers, conv_layers, norm_layer",,Autoformer encoder
neuralforecast.models.autoformer.EncoderLayer,type,"attention, hidden_size, conv_hidden_size, MovingAvg, dropout, activation",,Autoformer encoder layer with the progressive decomposition architecture
neuralforecast.models.autoformer.LayerNorm,type,channels,,Special designed layernorm for the seasonal part
neuralforecast.models.autoformer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.autoformer.SeriesDecomp,type,kernel_size,,Series decomposition block
neuralforecast.models.bitcn.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.bitcn.BiTCN,type,"h, input_size, hidden_size, dropout, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,BiTCN
neuralforecast.models.bitcn.CustomConv1d,type,"in_channels, out_channels, kernel_size, padding, dilation, mode, groups",,Forward- and backward looking Conv1D
neuralforecast.models.bitcn.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.bitcn.TCNCell,type,"in_channels, out_channels, kernel_size, padding, dilation, mode, groups, dropout",,"Temporal Convolutional Network Cell, consisting of CustomConv1D modules."
neuralforecast.models.deepar.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.deepar.Decoder,type,"in_features, out_features, hidden_size, hidden_layers",,Multi-Layer Perceptron Decoder
neuralforecast.models.deepar.DeepAR,type,"h, input_size, h_train, lstm_n_layers, lstm_hidden_size, lstm_dropout, decoder_hidden_layers, decoder_hidden_size, trajectory_samples, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepAR
neuralforecast.models.deepar.DistributionLoss,type,"distribution, level, quantiles, num_samples, return_params, horizon_weight, distribution_kwargs",,DistributionLoss
neuralforecast.models.deepar.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.deepnpts.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.deepnpts.DeepNPTS,type,"h, input_size, hidden_size, batch_norm, dropout, n_layers, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DeepNPTS
neuralforecast.models.deepnpts.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.dilated_rnn.AttentiveLSTMLayer,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.
neuralforecast.models.dilated_rnn.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.dilated_rnn.DRNN,type,"n_input, n_hidden, n_layers, dilations, dropout, cell_type, batch_first",,Base class for all neural network modules.
neuralforecast.models.dilated_rnn.DilatedRNN,type,"h, input_size, inference_input_size, cell_type, dilations, encoder_hidden_size, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DilatedRNN
neuralforecast.models.dilated_rnn.LSTMCell,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.
neuralforecast.models.dilated_rnn.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.dilated_rnn.MLP,type,"in_features, out_features, activation, hidden_size, num_layers, dropout",,Multi-Layer Perceptron Class
neuralforecast.models.dilated_rnn.ResLSTMCell,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.
neuralforecast.models.dilated_rnn.ResLSTMLayer,type,"input_size, hidden_size, dropout",,Base class for all neural network modules.
neuralforecast.models.dlinear.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.dlinear.DLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, moving_avg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,DLinear
neuralforecast.models.dlinear.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.dlinear.MovingAvg,type,"kernel_size, stride",,Moving average block to highlight the trend of time series
neuralforecast.models.dlinear.SeriesDecomp,type,kernel_size,,Series decomposition block
neuralforecast.models.fedformer.AutoCorrelationLayer,type,"correlation, hidden_size, n_head, d_keys, d_values",,Auto Correlation Layer
neuralforecast.models.fedformer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.fedformer.DataEmbedding,type,"c_in, exog_input_size, hidden_size, pos_embedding, dropout",,Base class for all neural network modules.
neuralforecast.models.fedformer.Decoder,type,"layers, norm_layer, projection",,FEDformer decoder
neuralforecast.models.fedformer.DecoderLayer,type,"self_attention, cross_attention, hidden_size, c_out, conv_hidden_size, MovingAvg, dropout, activation",,FEDformer decoder layer with the progressive decomposition architecture
neuralforecast.models.fedformer.Encoder,type,"attn_layers, conv_layers, norm_layer",,FEDformer encoder
neuralforecast.models.fedformer.EncoderLayer,type,"attention, hidden_size, conv_hidden_size, MovingAvg, dropout, activation",,FEDformer encoder layer with the progressive decomposition architecture
neuralforecast.models.fedformer.FEDformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, decoder_input_size_multiplier, version, modes, mode_select, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, MovingAvg_window, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,FEDformer
neuralforecast.models.fedformer.FourierBlock,type,"in_channels, out_channels, seq_len, modes, mode_select_method",,Fourier block
neuralforecast.models.fedformer.FourierCrossAttention,type,"in_channels, out_channels, seq_len_q, seq_len_kv, modes, mode_select_method, activation, policy",,Fourier Cross Attention layer
neuralforecast.models.fedformer.LayerNorm,type,channels,,Special designed layernorm for the seasonal part
neuralforecast.models.fedformer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.fedformer.SeriesDecomp,type,kernel_size,,Series decomposition block
neuralforecast.models.fedformer.get_frequency_modes,function,"seq_len, modes, mode_select_method",,Get modes on frequency domain:
neuralforecast.models.gru.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.gru.GRU,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,GRU
neuralforecast.models.gru.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.gru.MLP,type,"in_features, out_features, activation, hidden_size, num_layers, dropout",,Multi-Layer Perceptron Class
neuralforecast.models.hint.HINT,type,"h, S, model, reconciliation, alias",,HINT
neuralforecast.models.hint.get_bottomup_P,function,S,,BottomUp Reconciliation Matrix.
neuralforecast.models.hint.get_identity_P,function,S,,
neuralforecast.models.hint.get_mintrace_ols_P,function,S,,MinTraceOLS Reconciliation Matrix.
neuralforecast.models.hint.get_mintrace_wls_P,function,S,,MinTraceOLS Reconciliation Matrix.
neuralforecast.models.iTransformer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, n_heads, e_layers, d_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,iTransformer
neuralforecast.models.informer.AttentionLayer,type,"attention, hidden_size, n_heads, d_keys, d_values",,Base class for all neural network modules.
neuralforecast.models.informer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.informer.ConvLayer,type,c_in,,ConvLayer
neuralforecast.models.informer.DataEmbedding,type,"c_in, exog_input_size, hidden_size, pos_embedding, dropout",,Base class for all neural network modules.
neuralforecast.models.informer.Informer,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, factor, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, distil, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Informer
neuralforecast.models.informer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.informer.ProbAttention,type,"mask_flag, factor, scale, attention_dropout, output_attention",,ProbAttention
neuralforecast.models.informer.ProbMask,type,"B, H, L, index, scores, device",,ProbMask
neuralforecast.models.informer.TransDecoder,type,"layers, norm_layer, projection",,Base class for all neural network modules.
neuralforecast.models.informer.TransDecoderLayer,type,"self_attention, cross_attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.informer.TransEncoder,type,"attn_layers, conv_layers, norm_layer",,Base class for all neural network modules.
neuralforecast.models.informer.TransEncoderLayer,type,"attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.itransformer.AttentionLayer,type,"attention, hidden_size, n_heads, d_keys, d_values",,Base class for all neural network modules.
neuralforecast.models.itransformer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.itransformer.DataEmbedding_inverted,type,"c_in, hidden_size, dropout",,DataEmbedding_inverted
neuralforecast.models.itransformer.FullAttention,type,"mask_flag, factor, scale, attention_dropout, output_attention",,Base class for all neural network modules.
neuralforecast.models.itransformer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.itransformer.TransEncoder,type,"attn_layers, conv_layers, norm_layer",,Base class for all neural network modules.
neuralforecast.models.itransformer.TransEncoderLayer,type,"attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.itransformer.iTransformer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, n_heads, e_layers, d_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,iTransformer
neuralforecast.models.kan.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.kan.KAN,type,"h, input_size, grid_size, spline_order, scale_noise, scale_base, scale_spline, enable_standalone_scale_spline, grid_eps, grid_range, n_hidden_layers, hidden_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, dataloader_kwargs, trainer_kwargs",,KAN
neuralforecast.models.kan.KANLinear,type,"in_features, out_features, grid_size, spline_order, scale_noise, scale_base, scale_spline, enable_standalone_scale_spline, base_activation, grid_eps, grid_range",,KANLinear
neuralforecast.models.kan.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.lstm.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.lstm.LSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,LSTM
neuralforecast.models.lstm.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.lstm.MLP,type,"in_features, out_features, activation, hidden_size, num_layers, dropout",,Multi-Layer Perceptron Class
neuralforecast.models.mlp.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.mlp.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.mlp.MLP,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLP
neuralforecast.models.mlpmultivariate.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.mlpmultivariate.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.mlpmultivariate.MLPMultivariate,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, num_layers, hidden_size, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,MLPMultivariate
neuralforecast.models.nbeats.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.nbeats.IdentityBasis,type,"backcast_size, forecast_size, out_features",,Base class for all neural network modules.
neuralforecast.models.nbeats.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.nbeats.NBEATS,type,"h, input_size, n_harmonics, n_polynomials, n_basis, basis, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATS
neuralforecast.models.nbeats.NBEATSBlock,type,"input_size, n_theta, mlp_units, basis, dropout_prob, activation",,N-BEATS block which takes a basis function as an argument.
neuralforecast.models.nbeats.SeasonalityBasis,type,"harmonics, backcast_size, forecast_size, out_features",,Base class for all neural network modules.
neuralforecast.models.nbeats.TrendBasis,type,"n_basis, backcast_size, forecast_size, out_features, basis",,Base class for all neural network modules.
neuralforecast.models.nbeats.generate_changepoint_basis,function,"length, n_basis",,Generates changepoint basis functions with automatically spaced changepoints.
neuralforecast.models.nbeats.generate_chebyshev_basis,function,"length, n_basis",,Generates Chebyshev polynomial basis functions.
neuralforecast.models.nbeats.generate_legendre_basis,function,"length, n_basis",,Generates Legendre polynomial basis functions.
neuralforecast.models.nbeats.generate_linear_hat_basis,function,"length, n_basis",,
neuralforecast.models.nbeats.generate_piecewise_linear_basis,function,"length, n_basis",,Generates piecewise linear basis functions (linear splines).
neuralforecast.models.nbeats.generate_polynomial_basis,function,"length, n_basis",,Generates standard polynomial basis functions.
neuralforecast.models.nbeats.generate_spline_basis,function,"length, n_basis",,Generates cubic spline basis functions.
neuralforecast.models.nbeats.get_basis,function,"length, n_basis, basis",,
neuralforecast.models.nbeatsx.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.nbeatsx.ExogenousBasis,type,forecast_size,,Base class for all neural network modules.
neuralforecast.models.nbeatsx.IdentityBasis,type,"backcast_size, forecast_size, out_features",,Base class for all neural network modules.
neuralforecast.models.nbeatsx.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.nbeatsx.NBEATSBlock,type,"input_size, h, futr_input_size, hist_input_size, stat_input_size, n_theta, mlp_units, basis, dropout_prob, activation",,N-BEATS block which takes a basis function as an argument.
neuralforecast.models.nbeatsx.NBEATSx,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_harmonics, n_polynomials, stack_types, n_blocks, mlp_units, dropout_prob_theta, activation, shared_weights, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NBEATSx
neuralforecast.models.nbeatsx.SeasonalityBasis,type,"harmonics, backcast_size, forecast_size, out_features",,Base class for all neural network modules.
neuralforecast.models.nbeatsx.TrendBasis,type,"degree_of_polynomial, backcast_size, forecast_size, out_features",,Base class for all neural network modules.
neuralforecast.models.nhits.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.nhits.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.nhits.NHITS,type,"h, input_size, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, stack_types, n_blocks, mlp_units, n_pool_kernel_size, n_freq_downsample, pooling_mode, interpolation_mode, dropout_prob_theta, activation, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NHITS
neuralforecast.models.nhits.NHITSBlock,type,"input_size, h, n_theta, mlp_units, basis, futr_input_size, hist_input_size, stat_input_size, n_pool_kernel_size, pooling_mode, dropout_prob, activation",,NHITS block which takes a basis function as an argument.
neuralforecast.models.nlinear.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.nlinear.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.nlinear.NLinear,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,NLinear
neuralforecast.models.patchtst.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.patchtst.Coord1dPosEncoding,function,"q_len, exponential, normalize",,
neuralforecast.models.patchtst.Coord2dPosEncoding,function,"q_len, hidden_size, exponential, normalize, eps",,
neuralforecast.models.patchtst.Flatten_Head,type,"individual, n_vars, nf, h, c_out, head_dropout",,Flatten_Head
neuralforecast.models.patchtst.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.patchtst.PatchTST,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, encoder_layers, n_heads, hidden_size, linear_hidden_size, dropout, fc_dropout, head_dropout, attn_dropout, patch_len, stride, revin, revin_affine, revin_subtract_last, activation, res_attention, batch_normalization, learn_pos_embed, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,PatchTST
neuralforecast.models.patchtst.PatchTST_backbone,type,"c_in, c_out, input_size, h, patch_len, stride, max_seq_len, n_layers, hidden_size, n_heads, d_k, d_v, linear_hidden_size, norm, attn_dropout, dropout, act, key_padding_mask, padding_var, attn_mask, res_attention, pre_norm, store_attn, pe, learn_pe, fc_dropout, head_dropout, padding_patch, pretrain_head, head_type, individual, revin, affine, subtract_last",,PatchTST_backbone
neuralforecast.models.patchtst.PositionalEncoding,function,"q_len, hidden_size, normalize",,
neuralforecast.models.patchtst.RevIN,type,"num_features, eps, affine, subtract_last, non_norm",,RevIN (Reversible-Instance-Normalization)
neuralforecast.models.patchtst.SinCosPosEncoding,function,"q_len, hidden_size, normalize",,
neuralforecast.models.patchtst.TSTEncoder,type,"q_len, hidden_size, n_heads, d_k, d_v, linear_hidden_size, norm, attn_dropout, dropout, activation, res_attention, n_layers, pre_norm, store_attn",,TSTEncoder
neuralforecast.models.patchtst.TSTEncoderLayer,type,"q_len, hidden_size, n_heads, d_k, d_v, linear_hidden_size, store_attn, norm, attn_dropout, dropout, bias, activation, res_attention, pre_norm",,TSTEncoderLayer
neuralforecast.models.patchtst.TSTiEncoder,type,"c_in, patch_num, patch_len, max_seq_len, n_layers, hidden_size, n_heads, d_k, d_v, linear_hidden_size, norm, attn_dropout, dropout, act, store_attn, key_padding_mask, padding_var, attn_mask, res_attention, pre_norm, pe, learn_pe",,TSTiEncoder
neuralforecast.models.patchtst.Transpose,type,"dims, contiguous",,Transpose
neuralforecast.models.patchtst.get_activation_fn,function,activation,,
neuralforecast.models.patchtst.positional_encoding,function,"pe, learn_pe, q_len, hidden_size",,
neuralforecast.models.rmok.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.rmok.JacobiKANLayer,type,"input_dim, output_dim, degree, a, b",,https://github.com/SpaceLearner/JacobiKAN/blob/main/JacobiKANLayer.py
neuralforecast.models.rmok.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.rmok.RMoK,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, taylor_order, jacobi_degree, wavelet_function, dropout, revin_affine, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Reversible Mixture of KAN
neuralforecast.models.rmok.RevINMultivariate,type,"num_features, eps, affine, subtract_last, non_norm",,ReversibleInstanceNorm1d for Multivariate models
neuralforecast.models.rmok.TaylorKANLayer,type,"input_dim, out_dim, order, addbias",,https://github.com/Muyuzhierchengse/TaylorKAN/
neuralforecast.models.rmok.WaveKANLayer,type,"in_features, out_features, wavelet_type, with_bn, device",,This is a sample code for the simulations of the paper:
neuralforecast.models.rnn.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.rnn.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.rnn.MLP,type,"in_features, out_features, activation, hidden_size, num_layers, dropout",,Multi-Layer Perceptron Class
neuralforecast.models.rnn.RNN,type,"h, input_size, inference_input_size, h_train, encoder_n_layers, encoder_hidden_size, encoder_activation, encoder_bias, encoder_dropout, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,RNN
neuralforecast.models.softs.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.softs.DataEmbedding_inverted,type,"c_in, d_model, dropout",,Data Embedding
neuralforecast.models.softs.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.softs.SOFTS,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, hidden_size, d_core, e_layers, d_ff, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,SOFTS
neuralforecast.models.softs.STAD,type,"d_series, d_core",,STar Aggregate Dispatch Module
neuralforecast.models.softs.TransEncoder,type,"attn_layers, conv_layers, norm_layer",,Base class for all neural network modules.
neuralforecast.models.softs.TransEncoderLayer,type,"attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.stemgnn.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.stemgnn.GLU,type,"input_channel, output_channel",,GLU
neuralforecast.models.stemgnn.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.stemgnn.StemGNN,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_stacks, multi_layer, dropout_rate, leaky_rate, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,StemGNN
neuralforecast.models.stemgnn.StockBlockLayer,type,"time_step, unit, multi_layer, stack_cnt",,StockBlockLayer
neuralforecast.models.tcn.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.tcn.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.tcn.MLP,type,"in_features, out_features, activation, hidden_size, num_layers, dropout",,Multi-Layer Perceptron Class
neuralforecast.models.tcn.TCN,type,"h, input_size, inference_input_size, kernel_size, dilations, encoder_hidden_size, encoder_activation, context_size, decoder_hidden_size, decoder_layers, futr_exog_list, hist_exog_list, stat_exog_list, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TCN
neuralforecast.models.tcn.TemporalConvolutionEncoder,type,"in_channels, out_channels, kernel_size, dilations, activation",,Temporal Convolution Encoder
neuralforecast.models.tft.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.tft.GLU,type,"hidden_size, output_size",,Base class for all neural network modules.
neuralforecast.models.tft.GRN,type,"input_size, hidden_size, output_size, context_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.tft.InterpretableMultiHeadAttention,type,"n_head, hidden_size, example_length, attn_dropout, dropout",,Base class for all neural network modules.
neuralforecast.models.tft.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.tft.MaybeLayerNorm,type,"output_size, hidden_size, eps",,Base class for all neural network modules.
neuralforecast.models.tft.StaticCovariateEncoder,type,"hidden_size, num_static_vars, dropout, grn_activation, rnn_type, n_rnn_layers, one_rnn_initial_state",,Base class for all neural network modules.
neuralforecast.models.tft.TFT,type,"h, input_size, tgt_size, stat_exog_list, hist_exog_list, futr_exog_list, hidden_size, n_head, attn_dropout, grn_activation, n_rnn_layers, rnn_type, one_rnn_initial_state, dropout, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TFT
neuralforecast.models.tft.TFTEmbedding,type,"hidden_size, stat_input_size, futr_input_size, hist_input_size, tgt_size",,Base class for all neural network modules.
neuralforecast.models.tft.TemporalCovariateEncoder,type,"hidden_size, num_historic_vars, num_future_vars, dropout, grn_activation, rnn_type, n_rnn_layers",,Base class for all neural network modules.
neuralforecast.models.tft.TemporalFusionDecoder,type,"n_head, hidden_size, example_length, encoder_length, attn_dropout, dropout, grn_activation",,Base class for all neural network modules.
neuralforecast.models.tft.VariableSelectionNetwork,type,"hidden_size, num_inputs, dropout, grn_activation",,Base class for all neural network modules.
neuralforecast.models.tft.get_activation_fn,function,activation_str,typing.Callable,
neuralforecast.models.tide.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.tide.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.tide.MLPResidual,type,"input_dim, hidden_size, output_dim, dropout, layernorm",,MLPResidual
neuralforecast.models.tide.TiDE,type,"h, input_size, hidden_size, decoder_output_dim, temporal_decoder_dim, dropout, layernorm, num_encoder_layers, num_decoder_layers, temporal_width, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TiDE
neuralforecast.models.timellm.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.timellm.FlattenHead,type,"n_vars, nf, target_window, head_dropout",,FlattenHead
neuralforecast.models.timellm.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.timellm.PatchEmbedding,type,"d_model, patch_len, stride, dropout",,PatchEmbedding
neuralforecast.models.timellm.ReplicationPad1d,type,padding,,ReplicationPad1d
neuralforecast.models.timellm.ReprogrammingLayer,type,"d_model, n_heads, d_keys, d_llm, attention_dropout",,ReprogrammingLayer
neuralforecast.models.timellm.RevIN,type,"num_features, eps, affine, subtract_last, non_norm",,RevIN (Reversible-Instance-Normalization)
neuralforecast.models.timellm.TimeLLM,type,"h, input_size, patch_len, stride, d_ff, top_k, d_llm, d_model, n_heads, enc_in, dec_in, llm, llm_config, llm_tokenizer, llm_num_hidden_layers, llm_output_attention, llm_output_hidden_states, prompt_prefix, dropout, stat_exog_list, hist_exog_list, futr_exog_list, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeLLM
neuralforecast.models.timellm.TokenEmbedding,type,"c_in, d_model",,TokenEmbedding
neuralforecast.models.timemixer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.timemixer.DFT_series_decomp,type,top_k,,Series decomposition block
neuralforecast.models.timemixer.DataEmbedding_wo_pos,type,"c_in, d_model, dropout, embed_type, freq",,DataEmbedding_wo_pos
neuralforecast.models.timemixer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.timemixer.MultiScaleSeasonMixing,type,"seq_len, down_sampling_window, down_sampling_layers",,Bottom-up mixing season pattern
neuralforecast.models.timemixer.MultiScaleTrendMixing,type,"seq_len, down_sampling_window, down_sampling_layers",,Top-down mixing trend pattern
neuralforecast.models.timemixer.PastDecomposableMixing,type,"seq_len, pred_len, down_sampling_window, down_sampling_layers, d_model, dropout, channel_independence, decomp_method, d_ff, moving_avg, top_k",,PastDecomposableMixing
neuralforecast.models.timemixer.PositionalEmbedding,type,"hidden_size, max_len",,Base class for all neural network modules.
neuralforecast.models.timemixer.RevIN,type,"num_features, eps, affine, subtract_last, non_norm",,RevIN (Reversible-Instance-Normalization)
neuralforecast.models.timemixer.SeriesDecomp,type,kernel_size,,Series decomposition block
neuralforecast.models.timemixer.TemporalEmbedding,type,"d_model, embed_type, freq",,Base class for all neural network modules.
neuralforecast.models.timemixer.TimeMixer,type,"h, input_size, n_series, stat_exog_list, hist_exog_list, futr_exog_list, d_model, d_ff, dropout, e_layers, top_k, decomp_method, moving_avg, channel_independence, down_sampling_layers, down_sampling_window, down_sampling_method, use_norm, decoder_input_size_multiplier, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeMixer
neuralforecast.models.timemixer.TokenEmbedding,type,"c_in, hidden_size",,Base class for all neural network modules.
neuralforecast.models.timesnet.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.timesnet.DataEmbedding,type,"c_in, exog_input_size, hidden_size, pos_embedding, dropout",,Base class for all neural network modules.
neuralforecast.models.timesnet.FFT_for_Period,function,"x, k",,
neuralforecast.models.timesnet.Inception_Block_V1,type,"in_channels, out_channels, num_kernels, init_weight",,Inception_Block_V1
neuralforecast.models.timesnet.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.timesnet.TimesBlock,type,"input_size, h, k, hidden_size, conv_hidden_size, num_kernels",,TimesBlock
neuralforecast.models.timesnet.TimesNet,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, hidden_size, dropout, conv_hidden_size, top_k, num_kernels, encoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimesNet
neuralforecast.models.timexer.AttentionLayer,type,"attention, hidden_size, n_heads, d_keys, d_values",,Base class for all neural network modules.
neuralforecast.models.timexer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.timexer.DataEmbedding_inverted,type,"c_in, hidden_size, dropout",,DataEmbedding_inverted
neuralforecast.models.timexer.EnEmbedding,type,"n_vars, d_model, patch_len, dropout",,Base class for all neural network modules.
neuralforecast.models.timexer.Encoder,type,"layers, norm_layer, projection",,Base class for all neural network modules.
neuralforecast.models.timexer.EncoderLayer,type,"self_attention, cross_attention, d_model, d_ff, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.timexer.FlattenHead,type,"n_vars, nf, target_window, head_dropout",,Base class for all neural network modules.
neuralforecast.models.timexer.FullAttention,type,"mask_flag, factor, scale, attention_dropout, output_attention",,Base class for all neural network modules.
neuralforecast.models.timexer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.timexer.PositionalEmbedding,type,"hidden_size, max_len",,Base class for all neural network modules.
neuralforecast.models.timexer.TimeXer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, patch_len, hidden_size, n_heads, e_layers, d_ff, factor, dropout, use_norm, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TimeXer
neuralforecast.models.tsmixer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.tsmixer.FeatureMixing,type,"n_series, input_size, dropout, ff_dim",,FeatureMixing
neuralforecast.models.tsmixer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.tsmixer.MixingLayer,type,"n_series, input_size, dropout, ff_dim",,MixingLayer
neuralforecast.models.tsmixer.RevINMultivariate,type,"num_features, eps, affine, subtract_last, non_norm",,ReversibleInstanceNorm1d for Multivariate models
neuralforecast.models.tsmixer.TSMixer,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixer
neuralforecast.models.tsmixer.TemporalMixing,type,"n_series, input_size, dropout",,TemporalMixing
neuralforecast.models.tsmixerx.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.tsmixerx.FeatureMixing,type,"in_features, out_features, h, dropout, ff_dim",,FeatureMixing
neuralforecast.models.tsmixerx.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.tsmixerx.MixingLayer,type,"in_features, out_features, h, dropout, ff_dim",,MixingLayer
neuralforecast.models.tsmixerx.MixingLayerWithStaticExogenous,type,"h, dropout, ff_dim, stat_input_size",,MixingLayerWithStaticExogenous
neuralforecast.models.tsmixerx.RevINMultivariate,type,"num_features, eps, affine, subtract_last, non_norm",,ReversibleInstanceNorm1d for Multivariate models
neuralforecast.models.tsmixerx.ReversibleInstanceNorm1d,type,"n_series, eps",,Base class for all neural network modules.
neuralforecast.models.tsmixerx.TSMixerx,type,"h, input_size, n_series, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, n_block, ff_dim, dropout, revin, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,TSMixerx
neuralforecast.models.tsmixerx.TemporalMixing,type,"num_features, h, dropout",,TemporalMixing
neuralforecast.models.vanillatransformer.AttentionLayer,type,"attention, hidden_size, n_heads, d_keys, d_values",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.vanillatransformer.DataEmbedding,type,"c_in, exog_input_size, hidden_size, pos_embedding, dropout",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.FullAttention,type,"mask_flag, factor, scale, attention_dropout, output_attention",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.vanillatransformer.TransDecoder,type,"layers, norm_layer, projection",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.TransDecoderLayer,type,"self_attention, cross_attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.TransEncoder,type,"attn_layers, conv_layers, norm_layer",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.TransEncoderLayer,type,"attention, hidden_size, conv_hidden_size, dropout, activation",,Base class for all neural network modules.
neuralforecast.models.vanillatransformer.VanillaTransformer,type,"h, input_size, stat_exog_list, hist_exog_list, futr_exog_list, exclude_insample_y, decoder_input_size_multiplier, hidden_size, dropout, n_head, conv_hidden_size, activation, encoder_layers, decoder_layers, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,VanillaTransformer
neuralforecast.models.xLSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_blocks, encoder_hidden_size, encoder_bias, encoder_dropout, decoder_hidden_size, decoder_layers, decoder_dropout, decoder_activation, backbone, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,xLSTM
neuralforecast.models.xlstm.BaseModel,type,"h, input_size, loss, valid_loss, learning_rate, max_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, n_series, n_samples, h_train, inference_input_size, step_size, num_lr_decays, early_stop_patience_steps, scaler_type, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, drop_last_loader, random_seed, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,Hooks to be used in LightningModule.
neuralforecast.models.xlstm.MAE,type,horizon_weight,,Mean Absolute Error.
neuralforecast.models.xlstm.MLP,type,"in_features, out_features, activation, hidden_size, num_layers, dropout",,Multi-Layer Perceptron Class
neuralforecast.models.xlstm.xLSTM,type,"h, input_size, inference_input_size, h_train, encoder_n_blocks, encoder_hidden_size, encoder_bias, encoder_dropout, decoder_hidden_size, decoder_layers, decoder_dropout, decoder_activation, backbone, futr_exog_list, hist_exog_list, stat_exog_list, exclude_insample_y, recurrent, loss, valid_loss, max_steps, learning_rate, num_lr_decays, early_stop_patience_steps, val_check_steps, batch_size, valid_batch_size, windows_batch_size, inference_windows_batch_size, start_padding_enabled, training_data_availability_threshold, step_size, scaler_type, random_seed, drop_last_loader, alias, optimizer, optimizer_kwargs, lr_scheduler, lr_scheduler_kwargs, dataloader_kwargs, trainer_kwargs",,xLSTM
neuralforecast.tsdataset.BaseTimeSeriesDataset,type,"temporal_cols, max_size, min_size, y_idx, static, static_cols",,Base class for time series datasets.
neuralforecast.tsdataset.LocalFilesTimeSeriesDataset,type,"files_ds, temporal_cols, id_col, time_col, target_col, last_times, indices, max_size, min_size, y_idx, static, static_cols",,Time series dataset that loads data from local files.
neuralforecast.tsdataset.TimeSeriesDataModule,type,"dataset, batch_size, valid_batch_size, drop_last, shuffle_train, dataloaders_kwargs",,PyTorch Lightning data module for time series datasets.
neuralforecast.tsdataset.TimeSeriesDataset,type,"temporal, temporal_cols, indptr, y_idx, static, static_cols",,Time series dataset implementation.
neuralforecast.tsdataset.TimeSeriesLoader,type,"dataset, kwargs",,TimeSeriesLoader DataLoader.
neuralforecast.utils.DayOfMonth,type,,,"Day of month encoded as value between [-0.5, 0.5]."
neuralforecast.utils.DayOfWeek,type,,,"Day of week encoded as value between [-0.5, 0.5]."
neuralforecast.utils.DayOfYear,type,,,"Day of year encoded as value between [-0.5, 0.5]."
neuralforecast.utils.HourOfDay,type,,,"Hour of day encoded as value between [-0.5, 0.5]."
neuralforecast.utils.MinuteOfHour,type,,,"Minute of hour encoded as value between [-0.5, 0.5]."
neuralforecast.utils.MonthOfYear,type,,,"Month of year encoded as value between [-0.5, 0.5]."
neuralforecast.utils.PredictionIntervals,type,"n_windows, method",,Class for storing prediction intervals metadata information.
neuralforecast.utils.SecondOfMinute,type,,,"Second of minute encoded as value between [-0.5, 0.5]."
neuralforecast.utils.TimeFeature,type,,,
neuralforecast.utils.WeekOfYear,type,,,"Week of year encoded as value between [-0.5, 0.5]."
neuralforecast.utils.add_conformal_distribution_intervals,function,"model_fcsts, cs_df, model, cs_n_windows, n_series, horizon, level, quantiles","typing.Tuple[<built-in function array>, typing.List[str]]",Add conformal intervals based on conformal scores using distribution strategy.
neuralforecast.utils.add_conformal_error_intervals,function,"model_fcsts, cs_df, model, cs_n_windows, n_series, horizon, level, quantiles","typing.Tuple[<built-in function array>, typing.List[str]]",Add conformal intervals based on conformal scores using error strategy.
neuralforecast.utils.augment_calendar_df,function,"df, freq",,Augment a dataframe with calendar features based on frequency.
neuralforecast.utils.generate_series,function,"n_series, freq, min_length, max_length, n_temporal_features, n_static_features, equal_ends, seed",<class 'pandas.core.frame.DataFrame'>,Generate Synthetic Panel Series.
neuralforecast.utils.get_indexer_raise_missing,function,"idx, vals",typing.List[int],"Get index positions for values, raising error if any are missing."
neuralforecast.utils.get_prediction_interval_method,function,method,,Get the prediction interval method function by name.
neuralforecast.utils.level_to_quantiles,function,level,typing.List[float],Convert a list of confidence levels to quantiles.
neuralforecast.utils.quantiles_to_level,function,quantiles,"typing.List[typing.Union[int, float]]",Convert a list of quantiles to confidence levels.
neuralforecast.utils.time_features_from_frequency_str,function,freq_str,typing.List[neuralforecast.utils.TimeFeature],Returns a list of time features that will be appropriate for the given frequency string.
